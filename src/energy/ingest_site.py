#!/usr/bin/env python3
"""
Site Data Processing Pipeline
Processes site reference data and inserts into Iceberg site table
"""
import time
from pathlib import Path
from typing import Optional

from pyspark.sql import SparkSession
from pyspark.sql.types import DoubleType
from pyspark.sql.functions import lit
from src.utils.session import create_spark_session, SparkVersion
from src.utils.logger import HybridLogger
from src.utils.metrics import ErrorCode, Operation, OperationStatus
from src.utils.ingest import (
    InsertMode,
    calculate_job_metrics,
    check_table_exists,
    generate_job_id,
    insert_records,
    list_files,
    load_validation,
    read_files,
    safe_cast_columns,
)


def main(
    namespace: str,
    table_name: str,
    file_path_pattern: str,
    limit: Optional[int] = None,
    spark_session: Optional[SparkSession] = None,
):
    """Main function to process site data"""
    app_name = Path(__file__).stem

    # Use provided Spark session or create a new one
    if spark_session is not None:
        spark = spark_session
        should_stop_spark = False  # Don't stop session we didn't create
    else:
        spark = create_spark_session(
            spark_version=SparkVersion.SPARK_CONNECT_3_5,
            app_name=app_name,
            catalog="iceberg",
        )
        should_stop_spark = True  # Stop session we created

    try:
        job_id = generate_job_id()
        job_start_time = time.time()

        with HybridLogger(spark=spark, app_name=app_name, manage_spark=False) as logger:

            print(f"üè¢ Starting site data processing...")
            print(f"üìÅ Source: s3://raw/site")
            print(f"üéØ Target: energy.pv_site")
            print(f"üìã Job ID: {job_id}")

            try:
                print("üìã Loading site data files...")
                files_df, file_count = list_files(
                    spark, file_path_pattern, logger, limit
                )

                print(f"‚úÖ Found {file_count} site records")

                check_table_exists(spark, namespace, table_name, logger)

                files_df = read_files(spark, files_df, "parquet", {}, logger)

                processed_df = safe_cast_columns(
                    df=files_df,
                    columns=[
                        "av_pressure",
                        "av_temp",
                        "elevation",
                        "latitude",
                        "longitude",
                    ],
                    col_type=DoubleType,
                    logger=logger,
                )

                processed_df = processed_df.withColumn("job_id", lit(job_id))
                insert_results = insert_records(
                    namespace=namespace,
                    table_name=table_name,
                    data=processed_df,
                    hybrid_logger=logger,
                    mode=InsertMode.OVERWRITE,
                )

                load_validation(
                    spark,
                    namespace,
                    table_name,
                    insert_results["total_records"],
                    logger,
                )

                job_metrics = calculate_job_metrics(
                    insert_results=insert_results,
                    job_start_time=job_start_time,
                    hybrid_logger=logger,
                    files_count=files_df.count(),
                )

                print(f"‚úÖ Site data processing completed successfully")
                print(
                    f"üìä Job metrics: {job_metrics['job_status']} - {job_metrics['records_inserted']} records"
                )

                # Return job metrics for potential Dagster asset dependency
                return job_metrics

            except FileNotFoundError as e:
                logger.log(
                    Operation.FILE_READ.value,
                    OperationStatus.FAILURE.value,
                    {
                        "error_code": ErrorCode.FILE_NOT_FOUND.value,
                        "source_path": file_path_pattern,
                    },
                )
                raise
            except PermissionError as e:
                logger.log(
                    Operation.FILE_READ.value,
                    OperationStatus.FAILURE.value,
                    {
                        "error_code": ErrorCode.PERMISSION_DENIED.value,
                        "source_path": file_path_pattern,
                    },
                )
                raise
            except Exception as e:
                logger.log(
                    Operation.FILE_READ.value,
                    OperationStatus.ERROR.value,
                    {
                        "error_code": ErrorCode.PROCESSING_ERROR.value,
                        "context": "site_data_processing",
                    },
                )
                raise

    except Exception as e:
        print(f"‚ùå Error in site data processing: {e}")
        raise
    finally:
        if should_stop_spark:
            spark.stop()


if __name__ == "__main__":
    main(
        namespace="energy",
        table_name="pv_site",
        file_path_pattern="s3a://raw/site/*.parquet",
    )
