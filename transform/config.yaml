gateways:
  local:
    connection:
      type: duckdb
      register_comments: True
      pre_ping: False
      pretty_sql: True
      extensions:
        - httpfs
        - iceberg
        - aws
      secrets:
        - type: s3
          region: "us-east-1"
          key_id: "admin"
          secret: "password"
          endpoint: "http://localhost:9000"
          url_style: "path"
    state_connection:
      type: duckdb
      database: 'sqlmesh_state.db'
  
  spark:
    connection:
      type: spark
      config:
        spark.master: spark://localhost:7077
        spark.jars: /home/llee/projects/spark/transform/jars/iceberg-spark-runtime-3.5_2.12-1.9.1.jar,/home/llee/projects/spark/transform/jars/iceberg-aws-bundle-1.9.1.jar
        spark.sql.extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
        spark.sql.defaultCatalog: iceberg
        spark.sql.catalog.iceberg: org.apache.iceberg.spark.SparkCatalog
        spark.sql.catalog.iceberg.catalog-impl: org.apache.iceberg.rest.RESTCatalog
        spark.sql.catalog.iceberg.uri: http://localhost:8181
        spark.sql.catalog.iceberg.warehouse: s3://data/wh
        # S3 configuration for Iceberg catalog
        spark.sql.catalog.iceberg.s3.endpoint: http://localhost:9000
        spark.sql.catalog.iceberg.s3.access-key: admin
        spark.sql.catalog.iceberg.s3.secret-key: password
        spark.sql.catalog.iceberg.s3.region: us-east-1
        spark.sql.catalog.iceberg.s3.path-style-access: true
        spark.sql.catalog.iceberg.s3.ssl-enabled: false
        spark.sql.catalog.iceberg.s3.connection-timeout: 60000
        spark.sql.catalog.iceberg.s3.socket-timeout: 60000
        spark.sql.catalog.iceberg.s3.max-connections: 100
        # Hadoop S3 configuration for general S3 access
        spark.hadoop.fs.s3.impl: org.apache.hadoop.fs.s3a.S3AFileSystem
        spark.hadoop.fs.s3a.access.key: admin
        spark.hadoop.fs.s3a.secret.key: password
        spark.hadoop.fs.s3a.endpoint: http://localhost:9000
        spark.hadoop.fs.s3a.region: us-east-1
        spark.hadoop.fs.s3a.path.style.access: true
        spark.hadoop.fs.s3a.connection.ssl.enabled: false
        spark.hadoop.fs.s3a.aws.credentials.provider: org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
        spark.hadoop.fs.s3a.connection.maximum: 100
        spark.hadoop.fs.s3a.connection.timeout: 60000
        spark.hadoop.fs.s3a.socket.timeout: 60000
        spark.hadoop.fs.s3a.max.connections: 100
        spark.hadoop.fs.s3a.threads.max: 20
        spark.hadoop.fs.s3a.threads.core: 10
        spark.hadoop.fs.s3a.buffer.dir: /tmp
        spark.hadoop.fs.s3a.experimental.input.fadvise: normal
        spark.hadoop.fs.s3a.metrics.enabled: true
        spark.hadoop.fs.s3a.metrics.reporting.interval: 60
    test_connection:
      type: spark
      config:
        spark.master: local[*]
        spark.sql.adaptive.coalescePartitions.enabled: true
    state_connection:
      type: duckdb
      database: 'sqlmesh_spark_state.db'
  
  prod:
    connection:
      type: postgres
      host: localhost
      port: 5432
      user: admin 
      password: password
      database: iceberg
      register_comments: True
      pre_ping: False
      pretty_sql: True
    state_connection:
      type: postgres
      host: localhost
      port: 5432
      user: admin
      password: password
      database: sqlmesh 

default_gateway: spark

model_defaults:
  dialect: spark

ignore_patterns:
  - "generator"