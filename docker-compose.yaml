services:
  spark-master: 
    image: bitnami/spark:3.5.6
    container_name: spark-master
    hostname: spark-master
    networks:
      - spark-network
    env_file:
      - .env
    environment:
      - SPARK_HOME=/opt/bitnami/spark
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_MASTER_PORT=7077
      - SPARK_LOCAL_IP=spark-master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_WORKLOAD=master
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
      - SPARK_WEBUI_PORT=8080
      - SPARK_MASTER_LOG=/opt/bitnami/spark/logs/spark-master.out
      - SPARK_WORKER_LOG=/opt/bitnami/spark/logs/spark-worker.out
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_VERSION=3.5.6
      - SPARK_MAJOR_VERSION=3.5
    healthcheck:
      test: ["CMD", "pgrep", "-f", "org.apache.spark.deploy.master.Master"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    volumes:
      - ./spark-logs:/opt/bitnami/spark/logs
      - ./spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./log4j.properties:/opt/bitnami/spark/conf/log4j.properties
    ports:
      - "8080:8080"
      - "7077:7077"
    command: /opt/bitnami/spark/bin/spark-class org.apache.spark.deploy.master.Master --host spark-master --port 7077 --webui-port 8080
  spark-history:
    image: bitnami/spark:3.5.6
    container_name: spark-history
    depends_on:
      spark-master:
        condition: service_healthy
    networks:
      - spark-network
    volumes:
      - ./spark-logs:/opt/bitnami/spark/logs
      - ./spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./log4j.properties:/opt/bitnami/spark/conf/log4j.properties
    ports:
      - "18080:18080"
    environment:
      - SPARK_HOME=/opt/bitnami/spark
      - SPARK_VERSION=3.5.6
      - SPARK_MAJOR_VERSION=3.5
      - SPARK_HISTORY_OPTS=-Dspark.history.fs.logDirectory=file:///opt/bitnami/spark/logs
    command: /opt/bitnami/spark/bin/spark-class org.apache.spark.deploy.history.HistoryServer
  spark-connect:
    container_name: spark-connect
    build: 
      context: .
      dockerfile: docker/spark-connect
    depends_on:
      spark-master:
        condition: service_healthy
    networks:
      - spark-network
    environment:
      - SPARK_HOME=/opt/bitnami/spark
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_CONNECT_PORT=15002
      - SPARK_VERSION=3.5.6
      - SPARK_MAJOR_VERSION=3.5
      # Spark Connect configuration
      - SPARK_CONNECT_GRPC_BINDING_PORT=15002
      - SPARK_CONNECT_GRPC_BINDING_HOST=0.0.0.0
      - SPARK_CONNECT_GRPC_ARROW_ENABLED=true
      # Spark Connect server configuration
      - SPARK_CONNECT_SERVER_PORT=15002
      - SPARK_CONNECT_SERVER_HOST=0.0.0.0
      - AWS_REGION=us-east-1
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
    volumes:
      - ./spark-logs:/opt/bitnami/spark/logs
      - ./spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./log4j.properties:/opt/bitnami/spark/conf/log4j.properties
    ports:
      - "15002:15002"
    command: spark-submit --class org.apache.spark.sql.connect.service.SparkConnectServer
  spark-worker:
    build: 
      context: .
      dockerfile: docker/spark-worker 
    depends_on:
      spark-master:
        condition: service_healthy
    networks:
      - spark-network
    environment:
      - SPARK_HOME=/opt/bitnami/spark
      - SPARK_MODE=worker
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_MASTER_HOST=spark-master
      - SPARK_WEBUI_PORT=8080
      - SPARK_MASTER_LOG=/opt/bitnami/spark/logs/spark-master.out
      - SPARK_WORKER_LOG=/opt/bitnami/spark/logs/spark-worker.out
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_DRIVER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_WORKLOAD=worker
      - SPARK_LOCAL_IP=spark-worker
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
      - SPARK_EXECUTOR_LOGS_DIR=/opt/bitnami/spark/logs/executor
      # Spark 3.5 specific configurations
      - SPARK_VERSION=3.5.6
      - SPARK_MAJOR_VERSION=3.5
      # Spark 3.5 performance optimizations
      - SPARK_SQL_ADAPTIVE_ENABLED=true
      - SPARK_SQL_ADAPTIVE_COALESCE_PARTITIONS_ENABLED=true
      - SPARK_SQL_ADAPTIVE_SKEW_JOIN_ENABLED=true
      - SPARK_SQL_ADAPTIVE_LOCAL_SHUFFLE_READER_ENABLED=true
    healthcheck:
      test: ["CMD", "netstat", "-tuln", "|", "grep", ":8081"]
      interval: 15s
      timeout: 10s
      retries: 3
      start_period: 45s
    volumes:
      - ./spark-logs:/opt/bitnami/spark/logs
      - ./spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./log4j.properties:/opt/bitnami/spark/conf/log4j.properties
    command: /opt/bitnami/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    scale: 2  # This creates 2 worker instances
  postgres:
    image: bitnami/postgresql:17.5.0
    container_name: spark-postgres
    networks:
      - spark-network
    environment:
      - POSTGRES_DB=iceberg
      - POSTGRES_USER=admin
      - POSTGRES_PASSWORD=password
      - POSTGRES_SHARED_PRELOAD_LIBRARIES=pg_stat_statements
      - POSTGRES_MAX_CONNECTIONS=50
      - POSTGRES_SHARED_BUFFERS=128MB
      - POSTGRES_EFFECTIVE_CACHE_SIZE=256MB
      - POSTGRES_MAINTENANCE_WORK_MEM=32MB
      - POSTGRES_CHECKPOINT_COMPLETION_TARGET=0.9
      - POSTGRES_WAL_BUFFERS=4MB
      - POSTGRES_DEFAULT_STATISTICS_TARGET=100
      - POSTGRES_RANDOM_PAGE_COST=1.1
      - POSTGRES_EFFECTIVE_IO_CONCURRENCY=200
      - POSTGRES_WORK_MEM=4MB
      - POSTGRES_MIN_WAL_SIZE=1GB
      - POSTGRES_MAX_WAL_SIZE=4GB
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/postgres_init.sql:/docker-entrypoint-initdb.d/01-init-sqlmesh-db.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U admin -d iceberg"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
  iceberg-rest:
    build:
      context: .
      dockerfile: docker/iceberg-rest
    container_name: iceberg-rest
    networks:
      - spark-network
    ports:
      - "8181:8181"
    environment:
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
      - CATALOG_WAREHOUSE=s3://iceberg/
      - CATALOG_IO__IMPL=org.apache.iceberg.aws.s3.S3FileIO
      - CATALOG_S3_ENDPOINT=http://minio:9000
      - CATALOG_S3_REGION=us-east-1
      - CATALOG_S3_ACCESS_KEY_ID=admin
      - CATALOG_S3_SECRET_ACCESS_KEY=password
      - CATALOG_S3_FORCE_PATH_STYLE=true
      - CATALOG_S3_SSL_ENABLED=false
      - CATALOG_S3_CONNECTION_TIMEOUT=60000
      - CATALOG_S3_SOCKET_TIMEOUT=60000
      - CATALOG_S3_MAX_CONNECTIONS=100
      # PostgreSQL catalog configuration
      - CATALOG_CATALOG__IMPL=org.apache.iceberg.jdbc.JdbcCatalog
      - CATALOG_URI=jdbc:postgresql://postgres:5432/iceberg
      - CATALOG_JDBC_USER=admin
      - CATALOG_JDBC_PASSWORD=password
      # Java options to include PostgreSQL JDBC driver
      - JAVA_OPTS=-cp "/usr/lib/iceberg-rest/*"
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
      mc:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8181/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
  minio:
    image: minio/minio
    container_name: spark-minio
    networks:
      spark-network:
        aliases:
          - iceberg.minio
          - raw.minio
          - archive.minio
          - logs.minio
    environment:
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=password
      - MINIO_DOMAIN=minio
    ports:
      - 9001:9001
      - 9000:9000
    command: ["server", "/data", "--console-address", ":9001"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
  mc:
    depends_on:
      minio:
        condition: service_healthy
    image: minio/mc
    container_name: spark-mc
    networks:
      - spark-network
    environment:
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
    healthcheck:
      test: ["CMD", "mc", "alias", "list"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    volumes:
      - ./scripts/init_minio.sh:/init_minio.sh
    entrypoint: |
      /bin/sh -c "
      chmod +x /init_minio.sh;
      /init_minio.sh;
      tail -f /dev/null"
    restart: unless-stopped
volumes:
  spark-logs:
  postgres_data:  # Add PostgreSQL volume
networks:
  spark-network:
    driver: bridge
