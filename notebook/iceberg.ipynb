{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8b0e34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import create_spark_session\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "68106e00",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\n[PARSE_SYNTAX_ERROR] Syntax error at or near 'legal': missing 'FUNCTIONS'.(line 1, pos 14)\n\n== SQL ==\nSHOW BRANCHES legal.documents_snapshot\n--------------^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParseException\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSHOW BRANCHES legal.documents_snapshot\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/spark/.venv/lib/python3.12/site-packages/pyspark/sql/connect/session.py:550\u001b[39m, in \u001b[36mSparkSession.sql\u001b[39m\u001b[34m(self, sqlQuery, args)\u001b[39m\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery: \u001b[38;5;28mstr\u001b[39m, args: Optional[Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], List]] = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[33m\"\u001b[39m\u001b[33mDataFrame\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    549\u001b[39m     cmd = SQL(sqlQuery, args)\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m     data, properties = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    551\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33msql_command_result\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m properties:\n\u001b[32m    552\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame.withPlan(CachedRelation(properties[\u001b[33m\"\u001b[39m\u001b[33msql_command_result\u001b[39m\u001b[33m\"\u001b[39m]), \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/spark/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:982\u001b[39m, in \u001b[36mSparkConnectClient.execute_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    980\u001b[39m     req.user_context.user_id = \u001b[38;5;28mself\u001b[39m._user_id\n\u001b[32m    981\u001b[39m req.plan.command.CopyFrom(command)\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m data, _, _, _, properties = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_and_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    984\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (data.to_pandas(), properties)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/spark/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1283\u001b[39m, in \u001b[36mSparkConnectClient._execute_and_fetch\u001b[39m\u001b[34m(self, req, self_destruct)\u001b[39m\n\u001b[32m   1280\u001b[39m schema: Optional[StructType] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1281\u001b[39m properties: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = {}\n\u001b[32m-> \u001b[39m\u001b[32m1283\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_and_fetch_as_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1284\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mStructType\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1285\u001b[39m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/spark/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1264\u001b[39m, in \u001b[36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[39m\u001b[34m(self, req)\u001b[39m\n\u001b[32m   1262\u001b[39m                     \u001b[38;5;28;01myield from\u001b[39;00m handle_response(b)\n\u001b[32m   1263\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/spark/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1503\u001b[39m, in \u001b[36mSparkConnectClient._handle_error\u001b[39m\u001b[34m(self, error)\u001b[39m\n\u001b[32m   1490\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1491\u001b[39m \u001b[33;03mHandle errors that occur during RPC calls.\u001b[39;00m\n\u001b[32m   1492\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1500\u001b[39m \u001b[33;03mThrows the appropriate internal Python exception.\u001b[39;00m\n\u001b[32m   1501\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc.RpcError):\n\u001b[32m-> \u001b[39m\u001b[32m1503\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_rpc_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[32m   1505\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mCannot invoke RPC\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mclosed\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/spark/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1539\u001b[39m, in \u001b[36mSparkConnectClient._handle_rpc_error\u001b[39m\u001b[34m(self, rpc_error)\u001b[39m\n\u001b[32m   1537\u001b[39m             info = error_details_pb2.ErrorInfo()\n\u001b[32m   1538\u001b[39m             d.Unpack(info)\n\u001b[32m-> \u001b[39m\u001b[32m1539\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m convert_exception(info, status.message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1541\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(status.message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1542\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mParseException\u001b[39m: \n[PARSE_SYNTAX_ERROR] Syntax error at or near 'legal': missing 'FUNCTIONS'.(line 1, pos 14)\n\n== SQL ==\nSHOW BRANCHES legal.documents_snapshot\n--------------^^^\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW BRANCHES legal.documents_snapshot\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b846c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------------+--------------------+--------+---------+------+--------------+--------------------+--------------------+--------------------+------------------+------------+\n",
      "|document_id|document_type|       generated_at|              source|language|file_size|method|schema_version|  metadata_file_path|            raw_text|           file_path|          batch_id|      job_id|\n",
      "+-----------+-------------+-------------------+--------------------+--------+---------+------+--------------+--------------------+--------------------+--------------------+------------------+------------+\n",
      "|    doc_000|        dummy|2024-01-01 10:00:00|soli_legal_docume...|      en|     1024| spark|           1.0|s3a://raw/docs/le...|This is a sample ...|s3a://raw/docs/le...|batch_20240115_001|test_job_001|\n",
      "+-----------+-------------+-------------------+--------------------+--------+---------+------+--------------+--------------------+--------------------+--------------------+------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from legal.documents_snapshot.branch_staging limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e05feb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+--------+---------+----------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|         document_id|document_type|        generated_at|              source|language|file_size|    method|schema_version|  metadata_file_path|            raw_text|           file_path|            batch_id|              job_id|\n",
      "+--------------------+-------------+--------------------+--------------------+--------+---------+----------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|04d7a600-a2db-453...|     contract|2025-07-07 16:12:...|soli_legal_docume...|      en|     1756|sequential|            v1|s3a://raw/docs/le...|LEGAL DOCUMENT 00...|s3a://raw/docs/le...|batch_legal_inser...|legal_insert_1751...|\n",
      "|77130b22-0fb5-482...|     contract|2025-07-07 16:12:...|soli_legal_docume...|      en|     1746|sequential|            v1|s3a://raw/docs/le...|LEGAL DOCUMENT 00...|s3a://raw/docs/le...|batch_legal_inser...|legal_insert_1751...|\n",
      "|7aa31dec-dfdd-473...|     contract|2025-07-07 16:12:...|soli_legal_docume...|      en|     1702|sequential|            v1|s3a://raw/docs/le...|LEGAL DOCUMENT 01...|s3a://raw/docs/le...|batch_legal_inser...|legal_insert_1751...|\n",
      "|f427ed77-f397-475...|     contract|2025-07-07 16:12:...|soli_legal_docume...|      en|     1693|sequential|            v1|s3a://raw/docs/le...|LEGAL DOCUMENT 00...|s3a://raw/docs/le...|batch_legal_inser...|legal_insert_1751...|\n",
      "|b5e91697-9c67-44a...|     contract|2025-07-07 16:12:...|soli_legal_docume...|      en|     1662|sequential|            v1|s3a://raw/docs/le...|LEGAL DOCUMENT 00...|s3a://raw/docs/le...|batch_legal_inser...|legal_insert_1751...|\n",
      "|b1be9f3d-fe88-40e...|     contract|2025-07-07 16:12:...|soli_legal_docume...|      en|     1660|sequential|            v1|s3a://raw/docs/le...|LEGAL DOCUMENT 00...|s3a://raw/docs/le...|batch_legal_inser...|legal_insert_1751...|\n",
      "|498819e9-4409-46d...|     contract|2025-07-07 16:12:...|soli_legal_docume...|      en|     1654|sequential|            v1|s3a://raw/docs/le...|LEGAL DOCUMENT 00...|s3a://raw/docs/le...|batch_legal_inser...|legal_insert_1751...|\n",
      "|dd819903-1ae6-4b8...|     contract|2025-07-07 16:12:...|soli_legal_docume...|      en|     1654|sequential|            v1|s3a://raw/docs/le...|LEGAL DOCUMENT 00...|s3a://raw/docs/le...|batch_legal_inser...|legal_insert_1751...|\n",
      "|bbaf9c1b-9297-478...|     contract|2025-07-07 16:12:...|soli_legal_docume...|      en|     1646|sequential|            v1|s3a://raw/docs/le...|LEGAL DOCUMENT 00...|s3a://raw/docs/le...|batch_legal_inser...|legal_insert_1751...|\n",
      "|ce71539c-8d12-46c...|     contract|2025-07-07 16:12:...|soli_legal_docume...|      en|     1645|sequential|            v1|s3a://raw/docs/le...|LEGAL DOCUMENT 00...|s3a://raw/docs/le...|batch_legal_inser...|legal_insert_1751...|\n",
      "+--------------------+-------------+--------------------+--------------------+--------+---------+----------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from legal.documents_snapshot limit 10\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "576e3a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------------------+-----------------------+---------------------+----------------------+\n",
      "|   name|  type|        snapshot_id|max_reference_age_in_ms|min_snapshots_to_keep|max_snapshot_age_in_ms|\n",
      "+-------+------+-------------------+-----------------------+---------------------+----------------------+\n",
      "|staging|BRANCH|2253963032283617332|                   NULL|                 NULL|                  NULL|\n",
      "|   main|BRANCH|6836768928478080649|                   NULL|                 NULL|                  NULL|\n",
      "+-------+------+-------------------+-----------------------+---------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from legal.documents_snapshot.refs\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e6cf3a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+--------------------+\n",
      "|          col_name|data_type|             comment|\n",
      "+------------------+---------+--------------------+\n",
      "|       document_id|   string|Unique identifier...|\n",
      "|     document_type|   string|Type of legal doc...|\n",
      "|      generated_at|timestamp|When the document...|\n",
      "|            source|   string|Source system ide...|\n",
      "|          language|   string|Language of the d...|\n",
      "|         file_size|   bigint|Size of the file ...|\n",
      "|            method|   string|Processing method...|\n",
      "|    schema_version|   string|Version of the me...|\n",
      "|metadata_file_path|   string|Path to the metad...|\n",
      "|          raw_text|   string|Raw text content ...|\n",
      "|         file_path|   string|Path to the docum...|\n",
      "|          batch_id|   string|Links to batch_jo...|\n",
      "|            job_id|   string|Job identifier fo...|\n",
      "+------------------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE TABLE legal.documents_snapshot\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06ca8033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------------+-----------------------------+--------+---------+------+--------------+------------------------------------------------------------+----------------------------------------------------------+----------------------------------------------------------+------------------+------------+\n",
      "|document_id|document_type|generated_at       |source                       |language|file_size|method|schema_version|metadata_file_path                                          |raw_text                                                  |file_path                                                 |batch_id          |job_id      |\n",
      "+-----------+-------------+-------------------+-----------------------------+--------+---------+------+--------------+------------------------------------------------------------+----------------------------------------------------------+----------------------------------------------------------+------------------+------------+\n",
      "|doc_000    |dummy        |2024-01-01 10:00:00|soli_legal_document_generator|en      |1024     |spark |1.0           |s3a://raw/docs/legal/contract/20240115/metadata/doc_001.json|This is a sample legal contract text for testing purposes.|s3a://raw/docs/legal/contract/20240115/content/doc_001.txt|batch_20240115_001|test_job_001|\n",
      "+-----------+-------------+-------------------+-----------------------------+--------+---------+------+--------------+------------------------------------------------------------+----------------------------------------------------------+----------------------------------------------------------+------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_branch = spark.read \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .option(\"branch\", \"staging\") \\\n",
    "    .load(\"legal.documents_snapshot\")\n",
    "\n",
    "df_branch.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52177e00",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\n[PARSE_SYNTAX_ERROR] Syntax error at or near 'BRANCH'.(line 1, pos 42)\n\n== SQL ==\nselect * from legal.documents_snapshot IN BRANCH staging limit 10\n------------------------------------------^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParseException\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mselect * from legal.documents_snapshot IN BRANCH staging limit 10\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/spark/.venv/lib/python3.12/site-packages/pyspark/sql/connect/session.py:550\u001b[39m, in \u001b[36mSparkSession.sql\u001b[39m\u001b[34m(self, sqlQuery, args)\u001b[39m\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery: \u001b[38;5;28mstr\u001b[39m, args: Optional[Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], List]] = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[33m\"\u001b[39m\u001b[33mDataFrame\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    549\u001b[39m     cmd = SQL(sqlQuery, args)\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m     data, properties = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    551\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33msql_command_result\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m properties:\n\u001b[32m    552\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame.withPlan(CachedRelation(properties[\u001b[33m\"\u001b[39m\u001b[33msql_command_result\u001b[39m\u001b[33m\"\u001b[39m]), \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/spark/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:982\u001b[39m, in \u001b[36mSparkConnectClient.execute_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    980\u001b[39m     req.user_context.user_id = \u001b[38;5;28mself\u001b[39m._user_id\n\u001b[32m    981\u001b[39m req.plan.command.CopyFrom(command)\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m data, _, _, _, properties = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_and_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    984\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (data.to_pandas(), properties)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/spark/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1283\u001b[39m, in \u001b[36mSparkConnectClient._execute_and_fetch\u001b[39m\u001b[34m(self, req, self_destruct)\u001b[39m\n\u001b[32m   1280\u001b[39m schema: Optional[StructType] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1281\u001b[39m properties: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = {}\n\u001b[32m-> \u001b[39m\u001b[32m1283\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_and_fetch_as_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1284\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mStructType\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1285\u001b[39m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/spark/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1264\u001b[39m, in \u001b[36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[39m\u001b[34m(self, req)\u001b[39m\n\u001b[32m   1262\u001b[39m                     \u001b[38;5;28;01myield from\u001b[39;00m handle_response(b)\n\u001b[32m   1263\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/spark/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1503\u001b[39m, in \u001b[36mSparkConnectClient._handle_error\u001b[39m\u001b[34m(self, error)\u001b[39m\n\u001b[32m   1490\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1491\u001b[39m \u001b[33;03mHandle errors that occur during RPC calls.\u001b[39;00m\n\u001b[32m   1492\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1500\u001b[39m \u001b[33;03mThrows the appropriate internal Python exception.\u001b[39;00m\n\u001b[32m   1501\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc.RpcError):\n\u001b[32m-> \u001b[39m\u001b[32m1503\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_rpc_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[32m   1505\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mCannot invoke RPC\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mclosed\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/spark/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1539\u001b[39m, in \u001b[36mSparkConnectClient._handle_rpc_error\u001b[39m\u001b[34m(self, rpc_error)\u001b[39m\n\u001b[32m   1537\u001b[39m             info = error_details_pb2.ErrorInfo()\n\u001b[32m   1538\u001b[39m             d.Unpack(info)\n\u001b[32m-> \u001b[39m\u001b[32m1539\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m convert_exception(info, status.message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1541\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(status.message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1542\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mParseException\u001b[39m: \n[PARSE_SYNTAX_ERROR] Syntax error at or near 'BRANCH'.(line 1, pos 42)\n\n== SQL ==\nselect * from legal.documents_snapshot IN BRANCH staging limit 10\n------------------------------------------^^^\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from legal.documents_snapshot IN BRANCH staging limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0b829cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|  document_type|\n",
      "+---------------+\n",
      "|  legal_opinion|\n",
      "|     legal_memo|\n",
      "|       contract|\n",
      "|   court_filing|\n",
      "|policy_document|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT distinct(document_type) FROM legal.documents\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "619786b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+--------+\n",
      "|              job_id|        batch_id|count(1)|\n",
      "+--------------------+----------------+--------+\n",
      "|legal_insert_1751...|batch_1751839771|     100|\n",
      "+--------------------+----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT job_id, batch_id, count(*) FROM legal.documents GROUP BY job_id, batch_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155536cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1435888384.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mresult = spark.sql(\"SELECT raw_text FROM legal.documents WHERE document_type = 'contract' LIMIT 1\").\u001b[39m\n                                                                                                        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"SELECT raw_text FROM legal.documents WHERE document_type = 'contract' LIMIT 1\")\n",
    "result.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
