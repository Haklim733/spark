{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f67748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import dotenv\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from src.utils.session import create_spark_session\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d475da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getenv(\"SPARK_MASTER_URL\"))\n",
    "print(os.getenv(\"SPARK_CONNECT_SERVER\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2781695",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session()\n",
    "spark.conf.set(\"spark.sql.defaultCatalog\", \"spark_catalog\") # necessary to read from s3\n",
    "print(f\"Connected to: {spark.conf.get('spark.remote')}\")\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark session type: {type(spark)}\")\n",
    "print(f\"Connected to: {spark.conf.get('spark.remote')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66d29c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define data paths and check availability\n",
    "data_paths = {\n",
    "    'site':  's3a://raw/site/*.parquet',\n",
    "    'system': 's3a://raw/system/*.parquet',\n",
    "    'pvdata': 's3a://raw/pvdata/**/*.parquet'\n",
    "}\n",
    "\n",
    "files_d = {\n",
    "    'site': None,\n",
    "    'system': None,\n",
    "    'pvdata': None\n",
    "}\n",
    "\n",
    "print(\"Checking data paths:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef592339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import s3fs\n",
    "\n",
    "# df = pd.read_parquet('s3://raw/site/part-00000-a1617b0d-de41-4ca8-8586-51326d82d03f-c000.snappy.parquet', \n",
    "#                      storage_options={'key': 'admin', 'secret': 'password', 'client_kwargs': {'endpoint_url': 'http://localhost:9000'}})\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2576d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_new = SparkSession.builder \\\n",
    "    .appName(\"TestS3A\") \\\n",
    "    .remote(\"sc://localhost:15002\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"password\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"in-memory\") \\\n",
    "    .config(\"spark.sql.defaultCatalog\", \"spark_catalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "print(\"Default Catalog:\", spark_new.conf.get(\"spark.sql.defaultCatalog\"))\n",
    "print(\"Catalog Implementation:\", spark_new.conf.get(\"spark.sql.catalogImplementation\"))\n",
    "\n",
    "# Try reading with the new session\n",
    "df = spark_new.read.parquet(\"s3a://raw/site/\")\n",
    "df.show(5, truncate=False)\n",
    "spark_new.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457d6603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Read site information\n",
    "print(\"Reading site information...\")\n",
    "start_time = time.time()\n",
    "\n",
    "sites_df = spark.read.format('parquet').load(data_paths['site'])\n",
    "\n",
    "read_time = time.time() - start_time\n",
    "print(f\"✅ Sites data read in {read_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\nSites Schema:\")\n",
    "sites_df.printSchema()\n",
    "\n",
    "print(\"\\nSites Sample Data:\")\n",
    "sites_df.show(5, truncate=False)\n",
    "\n",
    "print(f\"\\nTotal sites: {sites_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f9072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Read system information\n",
    "print(\"Reading system information...\")\n",
    "start_time = time.time()\n",
    "\n",
    "systems_df = spark.read.parquet(data_paths['system'])\n",
    "\n",
    "read_time = time.time() - start_time\n",
    "print(f\"✅ Systems data read in {read_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\nSystems Schema:\")\n",
    "systems_df.printSchema()\n",
    "\n",
    "print(\"\\nSystems Sample Data:\")\n",
    "systems_df.show(5, truncate=False)\n",
    "\n",
    "print(f\"\\nTotal systems: {systems_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b1c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Explore PV data structure\n",
    "print(\"Exploring PV data structure...\")\n",
    "\n",
    "try:\n",
    "    pv_sample = spark.read.parquet(f\"s3a://raw/pvdata/system_id=3/\")\n",
    "    print(f\"✅ Found system-id=2 with {pv_sample.count()} records\")\n",
    "    \n",
    "    print(\"\\nPV Data Schema:\")\n",
    "    pv_sample.printSchema()\n",
    "    \n",
    "    print(\"\\nPV Data Sample:\")\n",
    "    pv_sample.show(5, truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error reading PV data: {str(e)}\")\n",
    "    print(\"Trying alternative approach...\")\n",
    "    \n",
    "    try:\n",
    "        all_pv = spark.read.parquet(data_paths['pvdata'])\n",
    "        print(f\"✅ Found total PV data with {all_pv.count()} records\")\n",
    "        \n",
    "        print(\"\\nPV Data Schema:\")\n",
    "        all_pv.printSchema()\n",
    "        \n",
    "        print(\"\\nPV Data Sample:\")\n",
    "        all_pv.show(5, truncate=False)\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"❌ Error reading all PV data: {str(e2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54655445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Performance test - Read large PV data\n",
    "print(\"Performance Test: Reading 14GB PV Data\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "pv_df = spark.read.parquet(\"s3a://raw/pvdata/\")\n",
    "total_records = pv_df.count()\n",
    "\n",
    "read_time = time.time() - start_time\n",
    "print(f\"✅ PV data read in {read_time:.2f} seconds\")\n",
    "\n",
    "print(f\"✅ Total PV records: {total_records:,}\") #270 sec\n",
    "print(f\"✅ Average read speed: {total_records/read_time:,.0f} records/second\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77303095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Iceberg as the default catalog\n",
    "spark.conf.set(\"spark.sql.defaultCatalog\", \"iceberg\")\n",
    "\n",
    "# Now you can create Iceberg tables directly\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS pv_data (\n",
    "    site_id STRING,\n",
    "    year INT,\n",
    "    month INT,\n",
    "    day INT,\n",
    "    power DOUBLE\n",
    ") USING iceberg\n",
    "PARTITIONED BY (year, month, day)\n",
    "\"\"\")\n",
    "\n",
    "# Insert data\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO pv_data\n",
    "SELECT * FROM pv_temp\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e7ae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register DataFrame as temp view\n",
    "pv_df.createOrReplaceTempView(\"pv_temp\")\n",
    "\n",
    "# Create Iceberg table and insert data\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS pv_data (\n",
    "    site_id STRING,\n",
    "    year INT,\n",
    "    month INT,\n",
    "    day INT,\n",
    "    power DOUBLE,\n",
    "    -- Add other columns as needed\n",
    "    timestamp TIMESTAMP\n",
    ") USING iceberg\n",
    "PARTITIONED BY (year, month, day)\n",
    "\"\"\")\n",
    "\n",
    "# Insert data\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO pv_data\n",
    "SELECT * FROM pv_temp\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f8ff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Data quality check\n",
    "print(\"Data Quality Check\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "print(\"\\nNull value counts:\")\n",
    "for col in pv_df.columns:\n",
    "    null_count = pv_df.filter(col(col).isNull()).count()\n",
    "    if null_count > 0:\n",
    "        print(f\"  {col}: {null_count:,} nulls\")\n",
    "\n",
    "print(\"\\nDuplicate check:\")\n",
    "total_rows = pv_df.count()\n",
    "distinct_rows = pv_df.distinct().count()\n",
    "duplicates = total_rows - distinct_rows\n",
    "print(f\"  Total rows: {total_rows:,}\")\n",
    "print(f\"  Distinct rows: {distinct_rows:,}\")\n",
    "print(f\"  Duplicates: {duplicates:,}\")\n",
    "\n",
    "print(\"\\nData types:\")\n",
    "for field in pv_df.schema.fields:\n",
    "    print(f\"  {field.name}: {field.dataType}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4433571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Create temporary views for SQL\n",
    "sites_df.createOrReplaceTempView(\"sites\")\n",
    "systems_df.createOrReplaceTempView(\"system\")\n",
    "pv_df.createOrReplaceTempView(\"pvdata\")\n",
    "\n",
    "print(\"Created temporary views for SQL operations\")\n",
    "print(f\"  - sites: {sites_df.count()} records\")\n",
    "print(f\"  - systems: {systems_df.count()} records\")\n",
    "print(f\"  - pvdata: {pv_df.count()} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15faa4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Join data using Spark SQL\n",
    "print(\"Joining site, system, and PV data...\")\n",
    "start_time = time.time()\n",
    "\n",
    "joined_query = \"\"\"\n",
    "SELECT \n",
    "    s.site_id,\n",
    "    s.site_name,\n",
    "    sys.system_id,\n",
    "    sys.system_name,\n",
    "    pv.*\n",
    "FROM pvdata pv\n",
    "JOIN systems sys ON pv.system_id = sys.system_id\n",
    "JOIN sites s ON sys.site_id = s.site_id\n",
    "\"\"\"\n",
    "\n",
    "joined_df = spark.sql(joined_query)\n",
    "\n",
    "join_time = time.time() - start_time\n",
    "print(f\"✅ Join completed in {join_time:.2f} seconds\")\n",
    "\n",
    "joined_df.cache()\n",
    "\n",
    "count_start = time.time()\n",
    "joined_count = joined_df.count()\n",
    "count_time = time.time() - count_start\n",
    "\n",
    "print(f\"✅ Joined data records: {joined_count:,}\")\n",
    "print(f\"✅ Count operation took: {count_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\nJoined Data Sample:\")\n",
    "joined_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380db1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Create view for joined data and perform aggregations\n",
    "joined_df.createOrReplaceTempView(\"joined_data\")\n",
    "\n",
    "print(\"Performing aggregations...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Cell 12: Total energy production by site\n",
    "print(\"1. Total energy production by site:\")\n",
    "start_time = time.time()\n",
    "\n",
    "site_totals = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    site_id,\n",
    "    site_name,\n",
    "    SUM(energy_production) as total_energy,\n",
    "    AVG(energy_production) as avg_energy,\n",
    "    COUNT(*) as record_count\n",
    "FROM joined_data\n",
    "GROUP BY site_id, site_name\n",
    "ORDER BY total_energy DESC\n",
    "\"\"\")\n",
    "\n",
    "query_time = time.time() - start_time\n",
    "print(f\"✅ Query completed in {query_time:.2f} seconds\")\n",
    "\n",
    "site_totals.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dc5e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: System performance analysis\n",
    "print(\"2. System performance analysis:\")\n",
    "start_time = time.time()\n",
    "\n",
    "system_analysis = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    system_id,\n",
    "    system_name,\n",
    "    site_name,\n",
    "    SUM(energy_production) as total_energy,\n",
    "    AVG(energy_production) as avg_energy,\n",
    "    MAX(energy_production) as max_energy,\n",
    "    MIN(energy_production) as min_energy,\n",
    "    COUNT(*) as record_count\n",
    "FROM joined_data\n",
    "GROUP BY system_id, system_name, site_name\n",
    "ORDER BY total_energy DESC\n",
    "\"\"\")\n",
    "\n",
    "query_time = time.time() - start_time\n",
    "print(f\"✅ Query completed in {query_time:.2f} seconds\")\n",
    "\n",
    "system_analysis.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a573265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Time-based analysis\n",
    "print(\"3. Time-based energy production analysis:\")\n",
    "start_time = time.time()\n",
    "\n",
    "time_analysis = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    DATE(timestamp) as date,\n",
    "    HOUR(timestamp) as hour,\n",
    "    SUM(energy_production) as daily_energy,\n",
    "    AVG(energy_production) as avg_hourly_energy,\n",
    "    COUNT(*) as record_count\n",
    "FROM joined_data\n",
    "GROUP BY DATE(timestamp), HOUR(timestamp)\n",
    "ORDER BY date DESC, hour\n",
    "LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "query_time = time.time() - start_time\n",
    "print(f\"✅ Query completed in {query_time:.2f} seconds\")\n",
    "\n",
    "time_analysis.show(20, truncate=False)\n",
    "\n",
    "# Cell 15: Performance monitoring and summary\n",
    "print(\"Performance Summary\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "app_id = spark.sparkContext.applicationId\n",
    "print(f\"Application ID: {app_id}\")\n",
    "\n",
    "try:\n",
    "    executors = spark.sparkContext.statusTracker().getExecutorMetrics()\n",
    "    print(f\"Active executors: {len(executors) if executors else 'Unknown'}\")\n",
    "except:\n",
    "    print(\"Executor info not available\")\n",
    "\n",
    "print(f\"\\nMemory configuration:\")\n",
    "print(f\"  Driver memory: {spark.conf.get('spark.driver.memory', 'Not set')}\")\n",
    "print(f\"  Executor memory: {spark.conf.get('spark.executor.memory', 'Not set')}\")\n",
    "\n",
    "print(f\"\\nData sizes:\")\n",
    "print(f\"  Sites: {sites_df.count():,} records\")\n",
    "print(f\"  Systems: {systems_df.count():,} records\")\n",
    "print(f\"  PV Data: {pv_df.count():,} records\")\n",
    "print(f\"  Joined: {joined_df.count():,} records\")\n",
    "\n",
    "# Cell 16: Cleanup\n",
    "print(\"Cleaning up cached data...\")\n",
    "joined_df.unpersist()\n",
    "\n",
    "print(\"✅ Analysis complete!\")\n",
    "print(\"\\nKey findings:\")\n",
    "print(\"1. Data read performance\")\n",
    "print(\"2. Join operation efficiency\")\n",
    "print(\"3. Aggregation query performance\")\n",
    "print(\"4. Memory usage patterns\")\n",
    "print(\"5. Overall workflow efficiency\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
