services:
  spark-master: 
    container_name: spark-master
    hostname: spark-master
    build: .
    networks:
      - spark-network
    ports:
      - "8080:8080"
      - "7077:7077"
    env_file:
      - .env
    environment:
      - SPARK_HOME=/opt/bitnami/spark
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_MASTER_PORT=7077
      - SPARK_LOCAL_IP=spark-master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_WORKLOAD=master
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
      - SPARK_WEBUI_PORT=8080
      - SPARK_MASTER_LOG=/opt/bitnami/spark/logs/spark-master.out
      - SPARK_WORKER_LOG=/opt/bitnami/spark/logs/spark-worker.out
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      # Spark 3.5 specific configurations
      - SPARK_VERSION=3.5.6
      - SPARK_MAJOR_VERSION=3.5
    healthcheck:
      test: ["CMD", "pgrep", "-f", "org.apache.spark.deploy.master.Master"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    volumes:
      - ./src:/home/app/src
      - ./scripts:/home/app/scripts
      - ./tests:/home/app/tests
      - ./pyproject.toml:/home/app/pyproject.toml
      - ./spark-logs:/opt/bitnami/spark/logs
      - ./spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./data/warehouse:/data/warehouse
    command: /opt/bitnami/spark/bin/spark-class org.apache.spark.deploy.master.Master
  spark-history:
    container_name: spark-history
    build: .
    depends_on:
      spark-master:
        condition: service_healthy
    networks:
      - spark-network
    volumes:
      - ./src:/home/app/src
      - ./tests:/home/app/tests
      - ./scripts:/home/app/scripts
      - ./pyproject.toml:/home/app/pyproject.toml
      - ./spark-logs:/opt/bitnami/spark/logs
      - ./spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./data/warehouse:/data/warehouse
    ports:
      - "18080:18080"
    environment:
      - SPARK_HOME=/opt/bitnami/spark
      - SPARK_VERSION=3.5.6
      - SPARK_MAJOR_VERSION=3.5
      - SPARK_HISTORY_OPTS=-Dspark.history.fs.logDirectory=file:///opt/bitnami/spark/logs
    command: /opt/bitnami/spark/bin/spark-class org.apache.spark.deploy.history.HistoryServer
  spark-worker:
    image: bitnami/spark:3.5.6
    build: .
    depends_on:
      spark-master:
        condition: service_healthy
    networks:
      - spark-network
    environment:
      - SPARK_HOME=/opt/bitnami/spark
      - SPARK_MODE=worker
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_MASTER_HOST=spark-master
      - SPARK_WEBUI_PORT=8080
      - SPARK_MASTER_LOG=/opt/bitnami/spark/logs/spark-master.out
      - SPARK_WORKER_LOG=/opt/bitnami/spark/logs/spark-worker.out
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1G
      - SPARK_DRIVER_MEMORY=1G
      - SPARK_EXECUTOR_MEMORY=1G
      - SPARK_WORKLOAD=worker
      - SPARK_LOCAL_IP=spark-worker
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
      - SPARK_EXECUTOR_LOGS_DIR=/opt/bitnami/spark/logs/executor
      # Spark 3.5 specific configurations
      - SPARK_VERSION=3.5.6
      - SPARK_MAJOR_VERSION=3.5
      # Spark 3.5 performance optimizations
      - SPARK_SQL_ADAPTIVE_ENABLED=true
      - SPARK_SQL_ADAPTIVE_COALESCE_PARTITIONS_ENABLED=true
      - SPARK_SQL_ADAPTIVE_SKEW_JOIN_ENABLED=true
      - SPARK_SQL_ADAPTIVE_LOCAL_SHUFFLE_READER_ENABLED=true
    healthcheck:
      test: ["CMD", "netstat", "-tuln", "|", "grep", ":8081"]
      interval: 15s
      timeout: 10s
      retries: 3
      start_period: 45s
    volumes:
      - ./src:/home/app/src
      - ./scripts:/home/app/scripts
      - ./spark-logs:/opt/bitnami/spark/logs
      - ./spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
      - ./data/warehouse:/data/warehouse
    command: /opt/bitnami/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    scale: 2  # This creates 2 worker instances
  iceberg-rest:
    image: apache/iceberg-rest-fixture:1.9.1
    container_name: iceberg-rest
    networks:
      - spark-network
    ports:
      - "8181:8181"
    environment:
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
      - CATALOG_WAREHOUSE=s3://data/wh
      - CATALOG_IO__IMPL=org.apache.iceberg.aws.s3.S3FileIO
      - CATALOG_S3_ENDPOINT=http://minio:9000
      - CATALOG_S3_REGION=us-east-1
      - CATALOG_S3_ACCESS_KEY_ID=admin
      - CATALOG_S3_SECRET_ACCESS_KEY=password
      - CATALOG_S3_FORCE_PATH_STYLE=true
      - CATALOG_S3_SSL_ENABLED=false
      - CATALOG_S3_CONNECTION_TIMEOUT=60000
      - CATALOG_S3_SOCKET_TIMEOUT=60000
      - CATALOG_S3_MAX_CONNECTIONS=100
    depends_on:
      - minio
      - mc
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8181/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
  minio:
    image: minio/minio
    container_name: spark-minio
    networks:
      spark-network:
        aliases:
          - data.minio
    environment:
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=password
      - MINIO_DOMAIN=minio
    ports:
      - 9001:9001
      - 9000:9000
    command: ["server", "/data", "--console-address", ":9001"]
  mc:
    depends_on:
      - minio
    image: minio/mc
    container_name: spark-mc
    networks:
      - spark-network
    environment:
      - AWS_ACCESS_KEY_ID=admin
      - AWS_SECRET_ACCESS_KEY=password
      - AWS_REGION=us-east-1
    entrypoint: |
      /bin/sh -c "
      until (/usr/bin/mc config host add minio http://minio:9000 admin password) do echo '...waiting...' && sleep 1; done;
      /usr/bin/mc rm -r --force minio/data;
      /usr/bin/mc mb minio/data;
      /usr/bin/mc mb minio/data/docs;
      /usr/bin/mc mb minio/data/docs/legal;
      /usr/bin/mc mb minio/data/wh;
      /usr/bin/mc anonymous set public minio/data;
      set +o history;
      tail -f /dev/null"
volumes:
  spark-logs:
networks:
  spark-network:
    driver: bridge
