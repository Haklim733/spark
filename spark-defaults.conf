# Spark Connect configuration
spark.connect.grpc.binding.port 15002
spark.connect.grpc.binding.host 0.0.0.0
spark.connect.grpc.arrow.enabled true

# Log4j configuration for app-specific logging
spark.driver.extraJavaOptions -Dlog4j.configuration=file:///opt/bitnami/spark/conf/log4j.properties
spark.executor.extraJavaOptions -Dlog4j.configuration=file:///opt/bitnami/spark/conf/log4j.properties

# Serialization settings - use Java serializer for better compatibility
spark.serializer org.apache.spark.serializer.JavaSerializer
spark.kryo.registrationRequired false

# Network settings
spark.network.timeout 800s
spark.executor.heartbeatInterval 60s

# Arrow settings
spark.sql.execution.arrow.pyspark.enabled true
spark.sql.execution.arrow.pyspark.fallback.enabled true

# I. Resource Management (Memory and CPU) - 1GB per worker configuration
spark.driver.memory 1g
spark.executor.memory 1g
spark.executor.cores 1
spark.cores.max 2
spark.dynamicAllocation.enabled true
spark.dynamicAllocation.minExecutors 1
spark.dynamicAllocation.maxExecutors 2
spark.dynamicAllocation.initialExecutors 1

# II. Shuffle Behavior
spark.shuffle.service.enabled true
spark.shuffle.compress true
spark.shuffle.registration.timeout 5000
spark.shuffle.manager org.apache.spark.shuffle.sort.SortShuffleManager

# III. Network and I/O (S3/MinIO Specific)
spark.hadoop.fs.s3a.connection.timeout 50000
spark.hadoop.fs.s3a.socket.timeout 50000
spark.hadoop.fs.s3a.max.connections 100
spark.hadoop.fs.s3a.fast.upload.buffer disk
spark.hadoop.fs.s3a.fast.upload.active.blocks 4
spark.hadoop.fs.s3a.multipart.size 104857600
spark.hadoop.fs.s3a.threads.max 20
spark.hadoop.fs.s3a.threads.core 10
spark.hadoop.fs.s3a.buffer.dir /tmp
spark.hadoop.fs.s3a.metrics.enabled true
spark.hadoop.fs.s3a.metrics.reporting.interval 60

# IV. SQL Engine Defaults (adjusted for smaller cluster)
spark.sql.adaptive.enabled true
spark.sql.shuffle.partitions 4
spark.sql.files.maxPartitionBytes 67108864
spark.sql.autoBroadcastJoinThreshold 5242880

# V. Memory Management
spark.memory.fraction 0.6
spark.memory.storageFraction 0.5
 

# Iceberg configuration (critical for catalog operations)
spark.sql.extensions org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
spark.sql.catalog.iceberg org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.iceberg.io-impl org.apache.iceberg.aws.s3.S3FileIO
spark.sql.catalogImplementation in-memory
spark.sql.catalog.iceberg.s3.connection-timeout 60000
spark.sql.catalog.iceberg.s3.socket-timeout 60000
spark.sql.catalog.iceberg.s3.max-connections 100


# S3A Filesystem configuration for direct S3/MinIO access
spark.hadoop.fs.s3a.impl org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.aws.credentials.provider org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
# spark.hadoop.fs.s3a.connection.ssl.enabled false
# spark.hadoop.fs.s3a.ssl.enabled false
# spark.hadoop.fs.s3a.path.style.access true
spark.hadoop.fs.s3a.endpoint http://minio:9000
#read s3a / listStatus error without this even with env var in docker compose and in session.py

# Logging configuration moved to session.py for per-app control
spark.sql.streaming.checkpointLocation file:///opt/bitnami/spark/logs/checkpoints

# Event log settings (server-side, for Spark Connect and classic Spark)
# spark.eventLog.enabled true
spark.sql.adaptive.coalescePartitions.enabled true
spark.sql.adaptive.skewJoin.enabled true
spark.sql.adaptive.advisoryPartitionSizeInBytes 128m

# Add to your spark-defaults.conf
# JVM optimizations for small memory
spark.driver.extraJavaOptions -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:InitiatingHeapOccupancyPercent=35
spark.executor.extraJavaOptions -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:InitiatingHeapOccupancyPercent=35

# Memory management for small clusters
spark.memory.fraction 0.7
spark.memory.storageFraction 0.3
spark.memory.offHeap.enabled true
spark.memory.offHeap.size 128m

# SQL optimizations for small data
spark.sql.adaptive.localShuffleReader.enabled true
spark.sql.adaptive.advisoryPartitionSizeInBytes 32m
spark.sql.autoBroadcastJoinThreshold 1m
spark.sql.files.maxPartitionBytes 32m

# Enhanced S3 settings
spark.hadoop.fs.s3a.experimental.input.fadvise normal
spark.hadoop.fs.s3a.readahead.range 1M
spark.hadoop.fs.s3a.connection.ssl.enabled false
spark.hadoop.fs.s3a.path.style.access true
