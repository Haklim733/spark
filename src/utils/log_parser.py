#!/usr/bin/env python3
"""
Parse Spark Event Logs to extract shuffling metrics and performance data
Enhanced to work with HybridLogger and retrieve logs from multiple sources
"""

import json
import sys
import os
import glob
import re
from collections import defaultdict
import argparse
from typing import Dict, List, Any


# ANSI color codes for better readability
class Colors:
    HEADER = "\033[95m"
    OKBLUE = "\033[94m"
    OKCYAN = "\033[96m"
    OKGREEN = "\033[92m"
    WARNING = "\033[93m"
    FAIL = "\033[91m"
    ENDC = "\033[0m"
    BOLD = "\033[1m"
    UNDERLINE = "\033[4m"


class HybridLogRetriever:
    """Enhanced log retriever that works with HybridLogger output"""

    def __init__(self, spark_logs_dir: str = "./spark-logs"):
        self.spark_logs_dir = spark_logs_dir
        self.logs = {
            "executor_logs": [],
            "driver_logs": [],
            "custom_metrics": [],
            "business_events": [],
            "performance_metrics": [],
            "errors": [],
        }

    def find_log_files(self) -> Dict[str, List[str]]:
        """Find all log files in the spark-logs directory"""
        log_files = {
            "executor_stdout": [],
            "executor_stderr": [],
            "driver_logs": [],
            "event_logs": [],
            "worker_logs": [],
        }

        # Find executor stdout logs
        executor_stdout_pattern = os.path.join(
            self.spark_logs_dir, "**", "executor", "*", "stdout"
        )
        log_files["executor_stdout"] = glob.glob(
            executor_stdout_pattern, recursive=True
        )

        # Find executor stderr logs
        executor_stderr_pattern = os.path.join(
            self.spark_logs_dir, "**", "executor", "*", "stderr"
        )
        log_files["executor_stderr"] = glob.glob(
            executor_stderr_pattern, recursive=True
        )

        # Find driver logs
        driver_pattern = os.path.join(self.spark_logs_dir, "spark-driver.out")
        if os.path.exists(driver_pattern):
            log_files["driver_logs"] = [driver_pattern]

        # Find event logs (JSON format)
        event_pattern = os.path.join(self.spark_logs_dir, "*.json")
        log_files["event_logs"] = glob.glob(event_pattern)

        # Find worker logs
        worker_pattern = os.path.join(self.spark_logs_dir, "**", "spark-worker.out")
        log_files["worker_logs"] = glob.glob(worker_pattern, recursive=True)

        return log_files

    def parse_hybrid_logs(
        self, log_files: List[str]
    ) -> Dict[str, List[Dict[str, Any]]]:
        """Parse logs for HybridLogger output"""
        parsed_logs = {
            "custom_metrics": [],
            "business_events": [],
            "performance_metrics": [],
            "errors": [],
        }

        for log_file in log_files:
            try:
                with open(log_file, "r", encoding="utf-8", errors="ignore") as f:
                    content = f.read()

                lines = content.split("\n")
                for line_num, line in enumerate(lines, 1):
                    if line.strip():
                        # Parse different log types from HybridLogger
                        if "CUSTOM_METRICS:" in line:
                            try:
                                json_start = line.find("{")
                                if json_start != -1:
                                    json_str = line[json_start:]
                                    parsed_metrics = json.loads(json_str)
                                    parsed_logs["custom_metrics"].append(
                                        {
                                            "file": log_file,
                                            "line": line_num,
                                            "timestamp": self.extract_timestamp(line),
                                            "data": parsed_metrics,
                                        }
                                    )
                            except json.JSONDecodeError:
                                continue

                        elif "BUSINESS_EVENT:" in line:
                            try:
                                json_start = line.find("{")
                                if json_start != -1:
                                    json_str = line[json_start:]
                                    parsed_event = json.loads(json_str)
                                    parsed_logs["business_events"].append(
                                        {
                                            "file": log_file,
                                            "line": line_num,
                                            "timestamp": self.extract_timestamp(line),
                                            "data": parsed_event,
                                        }
                                    )
                            except json.JSONDecodeError:
                                continue

                        elif "PERFORMANCE_METRICS:" in line:
                            try:
                                json_start = line.find("{")
                                if json_start != -1:
                                    json_str = line[json_start:]
                                    parsed_perf = json.loads(json_str)
                                    parsed_logs["performance_metrics"].append(
                                        {
                                            "file": log_file,
                                            "line": line_num,
                                            "timestamp": self.extract_timestamp(line),
                                            "data": parsed_perf,
                                        }
                                    )
                            except json.JSONDecodeError:
                                continue

                        elif "ERROR:" in line:
                            try:
                                json_start = line.find("{")
                                if json_start != -1:
                                    json_str = line[json_start:]
                                    parsed_error = json.loads(json_str)
                                    parsed_logs["errors"].append(
                                        {
                                            "file": log_file,
                                            "line": line_num,
                                            "timestamp": self.extract_timestamp(line),
                                            "data": parsed_error,
                                        }
                                    )
                            except json.JSONDecodeError:
                                continue

            except Exception as e:
                print(f"Error reading {log_file}: {e}")

        return parsed_logs

    def extract_timestamp(self, line: str) -> str:
        """Extract timestamp from log line"""
        patterns = [
            r"(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})",
            r"(\d{2}/\d{2}/\d{4} \d{2}:\d{2}:\d{2})",
            r"(\d{2}:\d{2}:\d{2})",
        ]

        for pattern in patterns:
            match = re.search(pattern, line)
            if match:
                return match.group(1)

        return "unknown"

    def get_hybrid_logs_summary(self) -> Dict[str, Any]:
        """Get summary of HybridLogger output"""
        log_files = self.find_log_files()
        executor_logs = self.parse_hybrid_logs(log_files["executor_stdout"])

        summary = {
            "total_log_files": sum(len(files) for files in log_files.values()),
            "custom_metrics_count": len(executor_logs["custom_metrics"]),
            "business_events_count": len(executor_logs["business_events"]),
            "performance_metrics_count": len(executor_logs["performance_metrics"]),
            "errors_count": len(executor_logs["errors"]),
            "recent_logs": {
                "custom_metrics": (
                    executor_logs["custom_metrics"][-5:]
                    if executor_logs["custom_metrics"]
                    else []
                ),
                "business_events": (
                    executor_logs["business_events"][-5:]
                    if executor_logs["business_events"]
                    else []
                ),
                "performance_metrics": (
                    executor_logs["performance_metrics"][-5:]
                    if executor_logs["performance_metrics"]
                    else []
                ),
                "errors": (
                    executor_logs["errors"][-5:] if executor_logs["errors"] else []
                ),
            },
        }

        return summary


def print_header(title):
    """Print a formatted header"""
    print(f"\n{Colors.HEADER}{Colors.BOLD}{'='*60}")
    print(f"  {title}")
    print(f"{'='*60}{Colors.ENDC}")


def print_section(title):
    """Print a formatted section header"""
    print(f"\n{Colors.OKBLUE}{Colors.BOLD}üìä {title}{Colors.ENDC}")
    print(f"{Colors.OKBLUE}{'-'*50}{Colors.ENDC}")


def print_metric(label, value, unit="", is_good=True, is_warning=False):
    """Print a formatted metric"""
    color = (
        Colors.OKGREEN if is_good else (Colors.WARNING if is_warning else Colors.FAIL)
    )
    print(f"  {label}: {color}{value}{unit}{Colors.ENDC}")


def format_duration(ms):
    """Format duration in human readable format"""
    if ms < 1000:
        return f"{ms:.0f}ms"
    elif ms < 60000:
        return f"{ms/1000:.1f}s"
    else:
        return f"{ms/60000:.1f}min"


def format_bytes(bytes_val):
    """Format bytes in human readable format"""
    if bytes_val < 1024:
        return f"{bytes_val}B"
    elif bytes_val < 1024 * 1024:
        return f"{bytes_val/1024:.1f}KB"
    elif bytes_val < 1024 * 1024 * 1024:
        return f"{bytes_val/(1024*1024):.1f}MB"
    else:
        return f"{bytes_val/(1024*1024*1024):.1f}GB"


def display_hybrid_logs_summary(retriever: HybridLogRetriever):
    """Display summary of HybridLogger output"""
    summary = retriever.get_hybrid_logs_summary()

    print_header("HYBRID LOGGER SUMMARY")

    print(f"  üìÅ Total log files found: {summary['total_log_files']}")
    print(f"  üìä Custom metrics: {summary['custom_metrics_count']}")
    print(f"  üìã Business events: {summary['business_events_count']}")
    print(f"  ‚ö° Performance metrics: {summary['performance_metrics_count']}")
    print(f"  ‚ùå Errors: {summary['errors_count']}")

    # Display recent logs
    if summary["recent_logs"]["custom_metrics"]:
        print_section("RECENT CUSTOM METRICS")
        for log in summary["recent_logs"]["custom_metrics"]:
            data = log["data"]
            print(
                f"  üìä {data.get('Operation', 'Unknown')} - {data.get('Job Group', 'Unknown')}"
            )
            if "Metrics" in data:
                metrics = data["Metrics"]
                if "data_skew_ratio" in metrics:
                    skew = metrics["data_skew_ratio"]
                    if skew > 10:
                        print(
                            f"    Data Skew: {Colors.FAIL}üö® {skew:.1f}x (SEVERE){Colors.ENDC}"
                        )
                    elif skew > 5:
                        print(
                            f"    Data Skew: {Colors.WARNING}‚ö†Ô∏è  {skew:.1f}x (HIGH){Colors.ENDC}"
                        )
                    else:
                        print(
                            f"    Data Skew: {Colors.OKGREEN}‚úÖ {skew:.1f}x (LOW){Colors.ENDC}"
                        )

    if summary["recent_logs"]["business_events"]:
        print_section("RECENT BUSINESS EVENTS")
        for log in summary["recent_logs"]["business_events"]:
            data = log["data"]
            print(
                f"  üìã {data.get('event_type', 'Unknown')} - {data.get('job_id', 'Unknown')}"
            )

    if summary["recent_logs"]["performance_metrics"]:
        print_section("RECENT PERFORMANCE METRICS")
        for log in summary["recent_logs"]["performance_metrics"]:
            data = log["data"]
            operation = data.get("operation", "Unknown")
            exec_time = data.get("execution_time_ms", 0)
            if exec_time > 1000:
                print(
                    f"  ‚ö° {operation}: {Colors.FAIL}üö® {exec_time:.0f}ms (SLOW){Colors.ENDC}"
                )
            elif exec_time > 500:
                print(
                    f"  ‚ö° {operation}: {Colors.WARNING}‚ö†Ô∏è  {exec_time:.0f}ms (MODERATE){Colors.ENDC}"
                )
            else:
                print(
                    f"  ‚ö° {operation}: {Colors.OKGREEN}‚úÖ {exec_time:.0f}ms (FAST){Colors.ENDC}"
                )

    if summary["recent_logs"]["errors"]:
        print_section("RECENT ERRORS")
        for log in summary["recent_logs"]["errors"]:
            data = log["data"]
            print(
                f"  ‚ùå {data.get('error_type', 'Unknown')}: {data.get('error_message', 'Unknown')}"
            )


def parse_spark_logs(log_file_path, include_hybrid_logs=True):
    """Parse Spark event logs and extract key metrics"""

    print_header("SPARK PERFORMANCE ANALYSIS")
    print(f"üìÅ Analyzing log file: {log_file_path}")

    # Initialize hybrid log retriever if requested
    hybrid_retriever = None
    if include_hybrid_logs:
        hybrid_retriever = HybridLogRetriever()
        display_hybrid_logs_summary(hybrid_retriever)

    # Metrics storage
    jobs = {}
    stages = {}
    tasks = defaultdict(list)
    shuffle_metrics = defaultdict(dict)
    custom_metrics = {}  # Store custom metrics from our application

    # Summary statistics
    total_jobs = 0
    total_duration = 0
    total_shuffle_write = 0
    total_shuffle_read = 0
    issues_found = []

    try:
        with open(log_file_path, "r") as f:
            for line_num, line in enumerate(f, 1):
                try:
                    # Check for custom metrics lines first (HybridLogger format)
                    if line.strip().startswith("CUSTOM_METRICS:"):
                        custom_metrics_json = line.strip().replace(
                            "CUSTOM_METRICS: ", ""
                        )
                        custom_event = json.loads(custom_metrics_json)

                        job_group = custom_event.get("Job Group")
                        operation = custom_event.get("Operation")
                        metrics = custom_event.get("Metrics", {})

                        if job_group not in custom_metrics:
                            custom_metrics[job_group] = {}
                        custom_metrics[job_group][operation] = metrics
                        continue

                    elif line.strip().startswith("CUSTOM_COMPLETION:"):
                        completion_json = line.strip().replace(
                            "CUSTOM_COMPLETION: ", ""
                        )
                        completion_event = json.loads(completion_json)

                        job_group = completion_event.get("Job Group")
                        operation = completion_event.get("Operation")
                        execution_time = completion_event.get("Execution Time", 0)
                        result_count = completion_event.get("Result Count", 0)

                        if (
                            job_group in custom_metrics
                            and operation in custom_metrics[job_group]
                        ):
                            custom_metrics[job_group][operation].update(
                                {
                                    "execution_time": execution_time,
                                    "result_count": result_count,
                                }
                            )
                        continue

                    # Parse standard Spark events
                    event = json.loads(line.strip())
                    event_type = event.get("Event", "")

                    # Parse different event types
                    if event_type == "SparkListenerJobStart":
                        job_id = event.get("Job ID")
                        jobs[job_id] = {
                            "start_time": event.get("Submission Time"),
                            "stages": [
                                stage["Stage ID"]
                                for stage in event.get("Stage Infos", [])
                            ],
                            "description": event.get("Properties", {}).get(
                                "spark.job.description", "Unknown"
                            ),
                            "group_id": event.get("Properties", {}).get(
                                "spark.jobGroup.id", "Unknown"
                            ),
                        }
                        total_jobs += 1

                    elif event_type == "SparkListenerJobEnd":
                        job_id = event.get("Job ID")
                        if job_id in jobs:
                            jobs[job_id]["end_time"] = event.get("Completion Time")
                            jobs[job_id]["result"] = event.get("Job Result", {}).get(
                                "Result"
                            )

                    elif event_type == "SparkListenerStageCompleted":
                        stage_info = event.get("Stage Info", {})
                        stage_id = stage_info.get("Stage ID")
                        stages[stage_id] = {
                            "name": stage_info.get("Stage Name"),
                            "completion_time": stage_info.get("Completion Time"),
                            "submission_time": stage_info.get("Submission Time"),
                            "num_tasks": stage_info.get("Number of Tasks"),
                            "accumulables": stage_info.get("Accumulables", []),
                        }

                        # Extract shuffle metrics from accumulables
                        for acc in stage_info.get("Accumulables", []):
                            name = acc.get("Name", "")
                            value = acc.get("Value", 0)

                            # Convert value to int if it's a string
                            try:
                                if isinstance(value, str):
                                    value = int(value)
                                else:
                                    value = int(value)
                            except (ValueError, TypeError):
                                value = 0

                            if "shuffle" in name.lower():
                                shuffle_metrics[stage_id][name] = value

                    elif event_type == "SparkListenerTaskEnd":
                        task_info = event.get("Task Info", {})
                        stage_id = event.get("Stage ID")
                        task_id = task_info.get("Task ID")

                        # Extract task metrics
                        task_metrics = {
                            "task_id": task_id,
                            "executor_id": task_info.get("Executor ID"),
                            "duration": task_info.get("Finish Time", 0)
                            - task_info.get("Launch Time", 0),
                            "shuffle_read": 0,
                            "shuffle_write": 0,
                            "memory_spilled": 0,
                            "disk_spilled": 0,
                        }

                        # Extract shuffle and spill metrics from accumulables
                        for acc in task_info.get("Accumulables", []):
                            name = acc.get("Name", "")
                            value = acc.get("Value", 0)

                            # Convert value to int if it's a string
                            try:
                                if isinstance(value, str):
                                    value = int(value)
                                else:
                                    value = int(value)
                            except (ValueError, TypeError):
                                value = 0

                            if "shuffle.read" in name.lower():
                                task_metrics["shuffle_read"] += value
                            elif "shuffle.write" in name.lower():
                                task_metrics["shuffle_write"] += value
                            elif "memory.bytes.spilled" in name.lower():
                                task_metrics["memory_spilled"] = value
                            elif "disk.bytes.spilled" in name.lower():
                                task_metrics["disk_spilled"] = value

                        tasks[stage_id].append(task_metrics)

                except json.JSONDecodeError:
                    continue  # Skip malformed lines
                except Exception as e:
                    print(f"Error parsing line {line_num}: {e}")
                    continue

        # Calculate summary statistics
        for job_id, job_info in jobs.items():
            if "end_time" in job_info and "start_time" in job_info:
                duration = job_info["end_time"] - job_info["start_time"]
                total_duration += duration

        # Analyze the parsed data
        analyze_jobs_human_readable(
            jobs, stages, shuffle_metrics, tasks, issues_found, custom_metrics
        )

        # Print summary
        print_summary(total_jobs, total_duration, issues_found)

        # Display custom metrics from logs
        display_custom_metrics_from_logs(custom_metrics)

    except FileNotFoundError:
        print(f"{Colors.FAIL}‚ùå Log file not found: {log_file_path}{Colors.ENDC}")
        print("Available log files:")
        print("docker exec spark-master ls -la /opt/bitnami/spark/logs/ | grep app-")
    except Exception as e:
        print(f"{Colors.FAIL}‚ùå Error reading log file: {e}{Colors.ENDC}")
        import traceback

        traceback.print_exc()


def display_custom_metrics_from_logs(custom_metrics):
    """Display custom metrics found in the Spark logs"""
    if not custom_metrics:
        print(
            f"\n  {Colors.WARNING}üìã No custom metrics found in Spark logs{Colors.ENDC}"
        )
        print(f"    Custom metrics are now integrated into Spark logs during execution")
        print(f"    Use HybridLogger for better performance and structured logging")
        return

    print_header("CUSTOM METRICS FROM SPARK LOGS")

    for job_group, operations in custom_metrics.items():
        print(f"\n  {Colors.OKBLUE}{Colors.BOLD}üìä {job_group.upper()}:{Colors.ENDC}")

        for operation, data in operations.items():
            print(f"\n    {Colors.OKCYAN}üîß {operation}:{Colors.ENDC}")

            # Data skew analysis
            if "data_skew_ratio" in data and data["data_skew_ratio"] > 0:
                skew_ratio = data["data_skew_ratio"]
                if skew_ratio > 10:
                    print(
                        f"      üìà Data Skew: {Colors.FAIL}üö® {skew_ratio:.1f}x (SEVERE){Colors.ENDC}"
                    )
                elif skew_ratio > 5:
                    print(
                        f"      üìà Data Skew: {Colors.WARNING}‚ö†Ô∏è  {skew_ratio:.1f}x (HIGH){Colors.ENDC}"
                    )
                else:
                    print(
                        f"      üìà Data Skew: {Colors.OKGREEN}‚úÖ {skew_ratio:.1f}x (LOW){Colors.ENDC}"
                    )

            # Data volume metrics
            if "record_count" in data:
                print(f"      üìä Records: {data['record_count']:,}")
            if "unique_keys" in data:
                print(f"      üîë Unique Keys: {data['unique_keys']}")
            if "max_key_frequency" in data:
                print(f"      üìà Max Key Frequency: {data['max_key_frequency']}")

            # Performance metrics
            if "execution_time" in data:
                exec_time = data["execution_time"]
                if exec_time > 10:
                    print(
                        f"      ‚è±Ô∏è  Execution: {Colors.FAIL}üö® {exec_time}s (SLOW){Colors.ENDC}"
                    )
                elif exec_time > 5:
                    print(
                        f"      ‚è±Ô∏è  Execution: {Colors.WARNING}‚ö†Ô∏è  {exec_time}s (MODERATE){Colors.ENDC}"
                    )
                else:
                    print(
                        f"      ‚è±Ô∏è  Execution: {Colors.OKGREEN}‚úÖ {exec_time}s (FAST){Colors.ENDC}"
                    )

            # Configuration metrics
            if "shuffle_partitions" in data:
                partitions = data["shuffle_partitions"]
                if partitions > 1000:
                    print(
                        f"      üîß Shuffle Partitions: {Colors.WARNING}‚ö†Ô∏è  {partitions} (TOO MANY){Colors.ENDC}"
                    )
                elif partitions < 10:
                    print(
                        f"      üîß Shuffle Partitions: {Colors.WARNING}‚ö†Ô∏è  {partitions} (TOO FEW){Colors.ENDC}"
                    )
                else:
                    print(
                        f"      üîß Shuffle Partitions: {Colors.OKGREEN}‚úÖ {partitions}{Colors.ENDC}"
                    )

            if "execution_plan_complexity" in data:
                complexity = data["execution_plan_complexity"]
                if complexity == "high":
                    print(
                        f"      üß† Plan Complexity: {Colors.WARNING}‚ö†Ô∏è  {complexity.upper()}{Colors.ENDC}"
                    )
                else:
                    print(
                        f"      üß† Plan Complexity: {Colors.OKGREEN}‚úÖ {complexity.upper()}{Colors.ENDC}"
                    )

            # Memory metrics
            if "executor_memory" in data:
                print(f"      üñ•Ô∏è  Executor Memory: {data['executor_memory']}")
            if "driver_memory" in data:
                print(f"      üñ•Ô∏è  Driver Memory: {data['driver_memory']}")
            if "memory_fraction" in data:
                mem_frac = data["memory_fraction"]
                if mem_frac < 0.5:
                    print(
                        f"      üìä Memory Fraction: {Colors.WARNING}‚ö†Ô∏è  {mem_frac} (LOW){Colors.ENDC}"
                    )
                else:
                    print(
                        f"      üìä Memory Fraction: {Colors.OKGREEN}‚úÖ {mem_frac}{Colors.ENDC}"
                    )
            if "storage_fraction" in data:
                storage_frac = data["storage_fraction"]
                if storage_frac < 0.3:
                    print(
                        f"      üíæ Storage Fraction: {Colors.WARNING}‚ö†Ô∏è  {storage_frac} (LOW){Colors.ENDC}"
                    )
                else:
                    print(
                        f"      üíæ Storage Fraction: {Colors.OKGREEN}‚úÖ {storage_frac}{Colors.ENDC}"
                    )
            if "estimated_memory_mb" in data:
                est_mem = data["estimated_memory_mb"]
                if est_mem > 1000:  # > 1GB
                    print(
                        f"      üíæ Estimated Memory: {Colors.FAIL}üö® {est_mem:.1f}MB (HIGH){Colors.ENDC}"
                    )
                elif est_mem > 500:  # > 500MB
                    print(
                        f"      üíæ Estimated Memory: {Colors.WARNING}‚ö†Ô∏è  {est_mem:.1f}MB (MODERATE){Colors.ENDC}"
                    )
                else:
                    print(
                        f"      üíæ Estimated Memory: {Colors.OKGREEN}‚úÖ {est_mem:.1f}MB{Colors.ENDC}"
                    )


def analyze_jobs_human_readable(
    jobs, stages, shuffle_metrics, tasks, issues_found, custom_metrics=None
):
    """Analyze job performance in human-readable format with grouped stages and tasks"""
    if custom_metrics is None:
        custom_metrics = {}

    print_section("JOB PERFORMANCE SUMMARY")

    if not jobs:
        print("  No jobs found in log file")
        return

    # Sort jobs by duration (longest first)
    job_list = []
    function_summary = {}  # Track function execution summary

    for job_id, job_info in jobs.items():
        if "end_time" in job_info and "start_time" in job_info:
            duration = job_info["end_time"] - job_info["start_time"]
            job_list.append((job_id, job_info, duration))

    job_list.sort(key=lambda x: x[2], reverse=True)

    print(f"  üìà Total Jobs: {len(job_list)}")

    for i, (job_id, job_info, duration) in enumerate(job_list, 1):
        print(f"\n  {Colors.HEADER}{Colors.BOLD}{'='*50}{Colors.ENDC}")
        print(f"  {Colors.BOLD}Job #{i} (ID: {job_id}){Colors.ENDC}")
        print(f"  {Colors.HEADER}{'='*50}{Colors.ENDC}")

        # Duration analysis
        duration_sec = duration / 1000
        if duration_sec > 10:
            print_metric("‚è±Ô∏è  Duration", format_duration(duration), "", False)
            issues_found.append(
                f"Job {job_id} took {format_duration(duration)} - very slow!"
            )
        elif duration_sec > 5:
            print_metric("‚è±Ô∏è  Duration", format_duration(duration), "", True, True)
            issues_found.append(
                f"Job {job_id} took {format_duration(duration)} - could be optimized"
            )
        else:
            print_metric("‚è±Ô∏è  Duration", format_duration(duration), "", True)

        # Enhanced job details with function identification
        description = job_info.get("description", "Unknown")
        group_id = job_info.get("group_id", "Unknown")

        # Extract function name and operation details from description
        function_name = "Unknown"
        operation_details = ""

        if description != "Unknown":
            if "() - " in description:
                parts = description.split("() - ", 1)
                function_name = parts[0] + "()"
                operation_details = parts[1] if len(parts) > 1 else ""
            elif "()" in description:
                function_name = description.split("()")[0] + "()"
                operation_details = (
                    description.split("()", 1)[1] if "()" in description else ""
                )

            print(f"  üîß Operation: {Colors.OKCYAN}{description}{Colors.ENDC}")
            if operation_details:
                print(f"  üìã Details: {Colors.OKCYAN}{operation_details}{Colors.ENDC}")
        else:
            print(f"  üîß Operation: {Colors.WARNING}{description}{Colors.ENDC}")

        # Display job group information and function mapping
        if group_id != "Unknown":
            print(f"  üè∑Ô∏è  Group: {Colors.OKCYAN}{group_id}{Colors.ENDC}")

            # Map group IDs to function names for better identification
            group_to_function = {
                "data_generation": "generate_skewed_data()",
                "data_skew_demo": "demonstrate_data_skew()",
                "large_partitions_demo": "demonstrate_large_shuffle_partitions()",
                "small_partitions_demo": "demonstrate_small_shuffle_partitions()",
                "cartesian_demo": "demonstrate_cartesian_product()",
                "join_strategies_demo": "demonstrate_join_strategies()",
                "shuffle_join_demo": "demonstrate_join_strategies() - Shuffle Join",
                "broadcast_join_demo": "demonstrate_join_strategies() - Broadcast Join",
                "window_functions_demo": "demonstrate_window_shuffling()",
                "repartitioning_demo": "demonstrate_repartitioning()",
                "coalescing_demo": "demonstrate_coalescing()",
                "bucketing_demo": "demonstrate_bucketing()",
                "adaptive_query_demo": "demonstrate_adaptive_query_execution()",
                "shuffling_issues_demo": "demonstrate_shuffling_issues()",
            }

            if group_id in group_to_function:
                actual_function = group_to_function[group_id]
                # Only show function if it's different from what we extracted from description
                if function_name == "Unknown" or function_name != actual_function:
                    print(
                        f"  üìù Function: {Colors.OKBLUE}{actual_function}{Colors.ENDC}"
                    )
                function_name = (
                    actual_function  # Use the mapped function name for summary
                )

                # Track function summary
                if actual_function not in function_summary:
                    function_summary[actual_function] = {
                        "jobs": 0,
                        "total_duration": 0,
                        "job_ids": [],
                    }
                function_summary[actual_function]["jobs"] += 1
                function_summary[actual_function]["total_duration"] += duration
                function_summary[actual_function]["job_ids"].append(job_id)
        else:
            print(f"  üè∑Ô∏è  Group: {Colors.WARNING}No job group{Colors.ENDC}")
            # If we have a function name from description, show it
            if function_name != "Unknown":
                print(f"  üìù Function: {Colors.OKBLUE}{function_name}{Colors.ENDC}")

        result = job_info.get("result", "Unknown")
        if result == "JobSucceeded":
            print(f"  ‚úÖ Status: {Colors.OKGREEN}{result}{Colors.ENDC}")
        else:
            print(f"  ‚ùå Status: {Colors.FAIL}{result}{Colors.ENDC}")
            issues_found.append(f"Job {job_id} failed: {result}")

        stages_count = len(job_info.get("stages", []))
        print(f"  üìä Stages: {stages_count}")

        # Show stages for this job
        job_stages = job_info.get("stages", [])
        if job_stages:
            print(f"\n    {Colors.OKBLUE}üìã STAGES FOR JOB {job_id}:{Colors.ENDC}")
            print(f"    Expected stages: {job_stages}")
            print(f"    Available stages in log: {list(stages.keys())}")

            for stage_id in job_stages:
                if stage_id in stages:
                    analyze_stage_for_job(
                        stage_id, stages[stage_id], shuffle_metrics, tasks, issues_found
                    )
                else:
                    print(
                        f"\n      {Colors.WARNING}Stage {stage_id} - Not found in log data{Colors.ENDC}"
                    )
                    print(
                        f"        This stage may have been skipped or failed to complete"
                    )
        else:
            print(
                f"\n    {Colors.WARNING}No stages found for Job {job_id}{Colors.ENDC}"
            )

        print(f"  {Colors.HEADER}{'='*50}{Colors.ENDC}")

    # Print function execution summary
    print_function_summary(function_summary)


def analyze_stage_for_job(stage_id, stage_info, shuffle_metrics, tasks, issues_found):
    """Analyze a specific stage within a job context"""
    print(f"\n      {Colors.BOLD}Stage {stage_id}{Colors.ENDC}")

    # Duration analysis - handle missing timing data gracefully
    if "completion_time" in stage_info and "submission_time" in stage_info:
        duration = stage_info["completion_time"] - stage_info["submission_time"]
        duration_sec = duration / 1000

        if duration_sec > 5:
            print(
                f"        ‚è±Ô∏è  Duration: {Colors.FAIL}{format_duration(duration)}{Colors.ENDC}"
            )
        elif duration_sec > 2:
            print(
                f"        ‚è±Ô∏è  Duration: {Colors.WARNING}{format_duration(duration)}{Colors.ENDC}"
            )
        else:
            print(
                f"        ‚è±Ô∏è  Duration: {Colors.OKGREEN}{format_duration(duration)}{Colors.ENDC}"
            )
    else:
        print(
            f"        ‚è±Ô∏è  Duration: {Colors.WARNING}Incomplete timing data{Colors.ENDC}"
        )

    # Stage details
    name = stage_info.get("name", "Unknown")
    print(f"        üîß Operation: {Colors.OKCYAN}{name}{Colors.ENDC}")

    num_tasks = stage_info.get("num_tasks", 0)
    print(f"        üìä Tasks: {num_tasks}")

    # Shuffle metrics for this stage
    stage_shuffle = shuffle_metrics.get(stage_id, {})
    if stage_shuffle:
        print(f"        üîÑ Shuffle Activity:")

        # Key metrics to focus on for performance analysis
        key_metrics = {
            "shuffle bytes written": "Shuffle Write Size",
            "shuffle records written": "Shuffle Write Records",
            "shuffle write time": "Shuffle Write Time (ns)",
            "internal.metrics.shuffle.read.remoteBytesRead": "Shuffle Read Size",
            "internal.metrics.shuffle.read.recordsRead": "Shuffle Read Records",
            "internal.metrics.shuffle.read.fetchWaitTime": "Shuffle Read Wait Time",
            "internal.metrics.shuffle.read.remoteBlocksFetched": "Remote Blocks Fetched",
            "internal.metrics.shuffle.read.localBlocksFetched": "Local Blocks Fetched",
        }

        shuffle_write_bytes = 0
        shuffle_read_bytes = 0
        shuffle_write_time = 0
        shuffle_read_wait = 0

        for metric, value in stage_shuffle.items():
            if metric in key_metrics:
                display_name = key_metrics[metric]

                if "bytes" in metric.lower():
                    formatted_value = format_bytes(value)
                    if value > 10 * 1024 * 1024:  # > 10MB
                        print(
                            f"          {display_name}: {Colors.FAIL}üö® {formatted_value} (EXPENSIVE!){Colors.ENDC}"
                        )
                        issues_found.append(
                            f"Stage {stage_id} has very high shuffle: {formatted_value}"
                        )
                    elif value > 1024 * 1024:  # > 1MB
                        print(
                            f"          {display_name}: {Colors.WARNING}‚ö†Ô∏è  {formatted_value} (HIGH){Colors.ENDC}"
                        )
                        issues_found.append(
                            f"Stage {stage_id} has high shuffle: {formatted_value}"
                        )
                    else:
                        print(
                            f"          {display_name}: {Colors.OKGREEN}‚úÖ {formatted_value}{Colors.ENDC}"
                        )

                    # Track total bytes for summary
                    if "write" in metric:
                        shuffle_write_bytes = value
                    elif "read" in metric:
                        shuffle_read_bytes = value

                elif "time" in metric.lower() or "wait" in metric.lower():
                    if value > 100000000:  # > 100ms in nanoseconds
                        print(
                            f"          {display_name}: {Colors.FAIL}üö® {value:,}ns (SLOW!){Colors.ENDC}"
                        )
                        issues_found.append(
                            f"Stage {stage_id} has slow shuffle: {value:,}ns"
                        )
                    elif value > 10000000:  # > 10ms
                        print(
                            f"          {display_name}: {Colors.WARNING}‚ö†Ô∏è  {value:,}ns (SLOW){Colors.ENDC}"
                        )
                    else:
                        print(
                            f"          {display_name}: {Colors.OKGREEN}‚úÖ {value:,}ns{Colors.ENDC}"
                        )

                    # Track timing for summary
                    if "write" in metric:
                        shuffle_write_time = value
                    elif "wait" in metric:
                        shuffle_read_wait = value

                elif "blocks" in metric.lower():
                    if value > 10:
                        print(
                            f"          {display_name}: {Colors.WARNING}‚ö†Ô∏è  {value} blocks{Colors.ENDC}"
                        )
                    else:
                        print(
                            f"          {display_name}: {Colors.OKGREEN}‚úÖ {value} blocks{Colors.ENDC}"
                        )

                else:
                    print(f"          {display_name}: {value}")

        # Show shuffle performance summary
        if shuffle_write_bytes > 0 or shuffle_read_bytes > 0:
            print(f"\n          üìä Shuffle Performance Summary:")
            if shuffle_write_bytes > 0:
                write_mb = shuffle_write_bytes / (1024 * 1024)
                if write_mb > 10:
                    print(
                        f"            üì§ Write: {Colors.FAIL}üö® {write_mb:.1f}MB (EXPENSIVE!){Colors.ENDC}"
                    )
                elif write_mb > 1:
                    print(
                        f"            üì§ Write: {Colors.WARNING}‚ö†Ô∏è  {write_mb:.1f}MB (HIGH){Colors.ENDC}"
                    )
                else:
                    print(
                        f"            üì§ Write: {Colors.OKGREEN}‚úÖ {write_mb:.1f}MB{Colors.ENDC}"
                    )

            if shuffle_read_bytes > 0:
                read_mb = shuffle_read_bytes / (1024 * 1024)
                if read_mb > 10:
                    print(
                        f"            üì• Read: {Colors.FAIL}üö® {read_mb:.1f}MB (EXPENSIVE!){Colors.ENDC}"
                    )
                elif read_mb > 1:
                    print(
                        f"            üì• Read: {Colors.WARNING}‚ö†Ô∏è  {read_mb:.1f}MB (HIGH){Colors.ENDC}"
                    )
                else:
                    print(
                        f"            üì• Read: {Colors.OKGREEN}‚úÖ {read_mb:.1f}MB{Colors.ENDC}"
                    )

            if shuffle_write_time > 100000000:  # > 100ms
                print(
                    f"            ‚è±Ô∏è  Write Time: {Colors.FAIL}üö® {shuffle_write_time/1000000:.1f}ms (SLOW!){Colors.ENDC}"
                )
            elif shuffle_write_time > 10000000:  # > 10ms
                print(
                    f"            ‚è±Ô∏è  Write Time: {Colors.WARNING}‚ö†Ô∏è  {shuffle_write_time/1000000:.1f}ms{Colors.ENDC}"
                )

            if shuffle_read_wait > 100000000:  # > 100ms
                print(
                    f"            ‚è±Ô∏è  Read Wait: {Colors.FAIL}üö® {shuffle_read_wait/1000000:.1f}ms (SLOW!){Colors.ENDC}"
                )
            elif shuffle_read_wait > 10000000:  # > 10ms
                print(
                    f"            ‚è±Ô∏è  Read Wait: {Colors.WARNING}‚ö†Ô∏è  {shuffle_read_wait/1000000:.1f}ms{Colors.ENDC}"
                )
    else:
        print(f"        üîÑ Shuffle Activity: {Colors.OKGREEN}None{Colors.ENDC}")

    # Show tasks for this stage
    if stage_id in tasks and tasks[stage_id]:
        print(f"\n          {Colors.OKCYAN}üìã TASKS FOR STAGE {stage_id}:{Colors.ENDC}")
        analyze_tasks_for_stage(stage_id, tasks[stage_id], issues_found)
    else:
        print(
            f"          {Colors.WARNING}No task data available for Stage {stage_id}{Colors.ENDC}"
        )


def analyze_tasks_for_stage(stage_id, stage_tasks, issues_found):
    """Analyze tasks for a specific stage"""
    if not stage_tasks:
        return

    # Calculate statistics
    durations = [t["duration"] for t in stage_tasks]
    shuffle_reads = [t["shuffle_read"] for t in stage_tasks]
    shuffle_writes = [t["shuffle_write"] for t in stage_tasks]

    # Duration analysis
    avg_duration = sum(durations) / len(durations)
    max_duration = max(durations)
    min_duration = min(durations)

    print(f"            üìä Task Count: {len(stage_tasks)}")
    print(f"            ‚è±Ô∏è  Avg Duration: {format_duration(avg_duration)}")
    print(f"            ‚è±Ô∏è  Max Duration: {format_duration(max_duration)}")
    print(f"            ‚è±Ô∏è  Min Duration: {format_duration(min_duration)}")

    # Check for skew
    if len(durations) > 1:
        duration_skew = max_duration / avg_duration
        if duration_skew > 3:
            print(
                f"            {Colors.FAIL}‚ö†Ô∏è  SEVERE DURATION SKEW: {duration_skew:.1f}x{Colors.ENDC}"
            )
            issues_found.append(
                f"Stage {stage_id} has severe task skew: {duration_skew:.1f}x"
            )
        elif duration_skew > 2:
            print(
                f"            {Colors.WARNING}‚ö†Ô∏è  MODERATE DURATION SKEW: {duration_skew:.1f}x{Colors.ENDC}"
            )
            issues_found.append(
                f"Stage {stage_id} has moderate task skew: {duration_skew:.1f}x"
            )

    # Shuffle analysis
    total_shuffle_read = sum(shuffle_reads)
    total_shuffle_write = sum(shuffle_writes)

    if total_shuffle_read > 0:
        print(f"            üì• Total Shuffle Read: {format_bytes(total_shuffle_read)}")
    if total_shuffle_write > 0:
        print(
            f"            üì§ Total Shuffle Write: {format_bytes(total_shuffle_write)}"
        )

    # Check for spills
    spills = [
        t for t in stage_tasks if t["memory_spilled"] > 0 or t["disk_spilled"] > 0
    ]
    if spills:
        print(
            f"            {Colors.FAIL}‚ö†Ô∏è  MEMORY SPILLS: {len(spills)} tasks spilled{Colors.ENDC}"
        )
        issues_found.append(
            f"Stage {stage_id} has {len(spills)} tasks with memory spills"
        )


def print_summary(total_jobs, total_duration, issues_found):
    """Print a summary of the analysis"""
    print_header("ANALYSIS SUMMARY")

    print(f"  üìä Total Jobs Analyzed: {total_jobs}")
    print(f"  ‚è±Ô∏è  Total Execution Time: {format_duration(total_duration)}")

    # Group jobs by function for better overview
    print(f"\n  {Colors.OKBLUE}üìã FUNCTION EXECUTION SUMMARY:{Colors.ENDC}")

    # This will be populated by the job analysis
    function_summary = {}

    if issues_found:
        print(f"\n  {Colors.FAIL}üö® ISSUES FOUND ({len(issues_found)}):{Colors.ENDC}")
        for i, issue in enumerate(issues_found, 1):
            print(f"    {i}. {issue}")

        print(f"\n  {Colors.WARNING}üí° RECOMMENDATIONS:{Colors.ENDC}")
        print("    ‚Ä¢ Check Spark Web UI for detailed metrics")
        print("    ‚Ä¢ Consider repartitioning for skewed data")
        print("    ‚Ä¢ Optimize shuffle partitions")
        print("    ‚Ä¢ Use broadcast joins for small tables")
        print("    ‚Ä¢ Monitor memory usage and spills")
    else:
        print(f"\n  {Colors.OKGREEN}‚úÖ No major issues detected!{Colors.ENDC}")
        print("    Your Spark job appears to be running efficiently.")


def print_function_summary(function_summary):
    """Print function execution summary"""
    if not function_summary:
        return

    print(
        f"\n  {Colors.OKBLUE}{Colors.BOLD}üìã FUNCTION EXECUTION SUMMARY:{Colors.ENDC}"
    )
    print(f"  {Colors.OKBLUE}{'-'*50}{Colors.ENDC}")

    # Sort functions by total duration (longest first)
    sorted_functions = sorted(
        function_summary.items(), key=lambda x: x[1]["total_duration"], reverse=True
    )

    for function, summary in sorted_functions:
        duration_sec = summary["total_duration"] / 1000

        # Color code based on performance
        if duration_sec > 10:
            duration_color = Colors.FAIL
            performance_indicator = "üö® SLOW"
        elif duration_sec > 5:
            duration_color = Colors.WARNING
            performance_indicator = "‚ö†Ô∏è  MODERATE"
        else:
            duration_color = Colors.OKGREEN
            performance_indicator = "‚úÖ FAST"

        print(f"  {Colors.OKBLUE}{Colors.BOLD}{function}:{Colors.ENDC}")
        print(f"    üìä Jobs: {summary['jobs']}")
        print(
            f"    ‚è±Ô∏è  Total Time: {duration_color}{format_duration(summary['total_duration'])} {performance_indicator}{Colors.ENDC}"
        )
        print(f"    üìã Job IDs: {', '.join(map(str, summary['job_ids']))}")
        print()


def main():
    """Main function"""
    parser = argparse.ArgumentParser(
        description="Parse a Spark event log file for shuffling metrics and performance data. Enhanced to work with HybridLogger."
    )
    parser.add_argument(
        "--file-path",
        dest="file_path",
        help="Path to the Spark event log file to parse",
    )
    parser.add_argument(
        "log_file",
        nargs="?",
        help="(Optional) Path to the Spark event log file (positional)",
    )
    parser.add_argument(
        "--no-hybrid-logs",
        action="store_true",
        help="Disable hybrid logging analysis (only parse Spark event logs)",
    )
    parser.add_argument(
        "--hybrid-only",
        action="store_true",
        help="Only analyze hybrid logs (skip Spark event log parsing)",
    )
    parser.add_argument(
        "--logs-dir",
        default="./spark-logs",
        help="Directory containing Spark logs (default: ./spark-logs)",
    )
    args = parser.parse_args()

    # Priority: positional > --file-path > default
    log_file = args.log_file or args.file_path

    # Handle hybrid-only mode
    if args.hybrid_only:
        print_header("HYBRID LOGGER ANALYSIS ONLY")
        retriever = HybridLogRetriever(args.logs_dir)
        display_hybrid_logs_summary(retriever)
        return

    # Handle regular parsing with optional hybrid logs
    if not log_file:
        print(
            "Error: You must specify a log file path with --file-path or as a positional argument."
        )
        print("\nAvailable options:")
        print("  --hybrid-only: Analyze only hybrid logs (no Spark event log required)")
        print("  --no-hybrid-logs: Disable hybrid logging analysis")
        print("  --logs-dir: Specify logs directory (default: ./spark-logs)")
        parser.print_help()
        sys.exit(1)

    # Parse with hybrid logs enabled by default (unless --no-hybrid-logs is specified)
    include_hybrid_logs = not args.no_hybrid_logs
    parse_spark_logs(log_file, include_hybrid_logs=include_hybrid_logs)


if __name__ == "__main__":
    main()
