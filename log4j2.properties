# Rename this file to log4j2.properties
# Place this file in your $SPARK_HOME/conf/ directory or specify its path with -Dlog4j.configurationFile

# Root logger configuration
rootLogger.level = INFO
rootLogger.appenderRefs = console, file, hybrid
rootLogger.appenderRef.console.ref = console
rootLogger.appenderRef.file.ref = file
rootLogger.appenderRef.hybrid.ref = hybrid

# Console appender
appender.console.name = console
appender.console.type = Console
appender.console.layout.type = PatternLayout
appender.console.layout.pattern = %d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n

# File appender for persistent logs (app-specific path)
appender.file.name = file
appender.file.type = RollingFile
# fileName is the active log file
appender.file.fileName = ${sys:spark.app.log.file:-/opt/bitnami/spark/logs/spark-application.log}
# filePattern is the archived log file name pattern
appender.file.filePattern = ${sys:spark.app.log.file:-/opt/bitnami/spark/logs/spark-application.log}.%i
appender.file.layout.type = PatternLayout
appender.file.layout.pattern = %d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n
appender.file.policies.type = Policies
appender.file.policies.size.type = SizeBasedTriggeringPolicy
appender.file.policies.size.size = 100MB
appender.file.strategy.type = DefaultRolloverStrategy
appender.file.strategy.max = 10
# Optional: Set immediatelyFlush to true if you want logs written immediately, might impact performance
# appender.file.immediateFlush = true

# HybridLogger appender for structured observability data
appender.hybrid.name = hybrid
appender.hybrid.type = RollingFile
appender.hybrid.fileName = ${sys:spark.hybrid.log.file:-/opt/bitnami/spark/logs/hybrid-observability.log}
appender.hybrid.filePattern = ${sys:spark.hybrid.log.file:-/opt/bitnami/spark/logs/hybrid-observability.log}.%i
appender.hybrid.layout.type = PatternLayout
appender.hybrid.layout.pattern = %m%n
appender.hybrid.policies.type = Policies
appender.hybrid.policies.size.type = SizeBasedTriggeringPolicy
appender.hybrid.policies.size.size = 200MB
appender.hybrid.strategy.type = DefaultRolloverStrategy
appender.hybrid.strategy.max = 20
# appender.hybrid.immediateFlush = true

# Specific logger configurations for different components

# Spark core logging
logger.spark.name = org.apache.spark
logger.spark.level = WARN
logger.spark.additivity = false # Optional: Prevents messages from propagating to parent loggers (e.g., root)

logger.hadoop.name = org.apache.hadoop
logger.hadoop.level = WARN
logger.hadoop.additivity = false

logger.parquet.name = org.apache.parquet
logger.parquet.level = WARN
logger.parquet.additivity = false

logger.iceberg.name = org.apache.iceberg
logger.iceberg.level = INFO
logger.iceberg.additivity = false

# Custom application loggers for HybridLogger integration
logger.hybridlogger.name = HybridLogger
logger.hybridlogger.level = INFO
logger.hybridlogger.appenderRefs = hybrid
logger.hybridlogger.appenderRef.hybrid.ref = hybrid
logger.hybridlogger.additivity = false

logger.batchmetricslogger.name = BatchMetricsLogger
logger.batchmetricslogger.level = INFO
logger.batchmetricslogger.appenderRefs = hybrid
logger.batchmetricslogger.appenderRef.hybrid.ref = hybrid
logger.batchmetricslogger.additivity = false

logger.performanceoptimizedlogger.name = PerformanceOptimizedLogger
logger.performanceoptimizedlogger.level = INFO
logger.performanceoptimizedlogger.appenderRefs = hybrid
logger.performanceoptimizedlogger.appenderRef.hybrid.ref = hybrid
logger.performanceoptimizedlogger.additivity = false

# Capture all PERFORMANCE_METRICS, BUSINESS_EVENT, and CUSTOM_METRICS
# Note: For these custom "loggers" which seem to be more like categories,
# ensure your application code uses `Logger.getLogger("PERFORMANCE_METRICS")` etc.
logger.perfmetrics.name = PERFORMANCE_METRICS
logger.perfmetrics.level = INFO
logger.perfmetrics.appenderRefs = hybrid
logger.perfmetrics.appenderRef.hybrid.ref = hybrid
logger.perfmetrics.additivity = false

logger.businessevent.name = BUSINESS_EVENT
logger.businessevent.level = INFO
logger.businessevent.appenderRefs = hybrid
logger.businessevent.appenderRef.hybrid.ref = hybrid
logger.businessevent.additivity = false

logger.custommetrics.name = CUSTOM_METRICS
logger.custommetrics.level = INFO
logger.custommetrics.appenderRefs = hybrid
logger.custommetrics.appenderRef.hybrid.ref = hybrid
logger.custommetrics.additivity = false

logger.customcompletion.name = CUSTOM_COMPLETION
logger.customcompletion.level = INFO
logger.customcompletion.appenderRefs = hybrid
logger.customcompletion.appenderRef.hybrid.ref = hybrid
logger.customcompletion.additivity = false

logger.filesuccess.name = FILE_SUCCESS
logger.filesuccess.level = INFO
logger.filesuccess.appenderRefs = hybrid
logger.filesuccess.appenderRef.hybrid.ref = hybrid
logger.filesuccess.additivity = false

logger.filefailure.name = FILE_FAILURE
logger.filefailure.level = INFO
logger.filefailure.appenderRefs = hybrid
logger.filefailure.appenderRef.hybrid.ref = hybrid
logger.filefailure.additivity = false

logger.batchmetrics.name = BATCH_METRICS
logger.batchmetrics.level = INFO
logger.batchmetrics.appenderRefs = hybrid
logger.batchmetrics.appenderRef.hybrid.ref = hybrid
logger.batchmetrics.additivity = false

# SQL and DataFrame operations
logger.sql.name = org.apache.spark.sql
logger.sql.level = INFO
logger.sql.additivity = false

logger.sqlexecution.name = org.apache.spark.sql.execution
logger.sqlexecution.level = INFO
logger.sqlexecution.additivity = false

# Streaming operations
logger.sparkstreaming.name = org.apache.spark.streaming
logger.sparkstreaming.level = INFO
logger.sparkstreaming.additivity = false

# Reduce noise from other components
logger.eclipsejetty.name = org.eclipse.jetty
logger.eclipsejetty.level = WARN
logger.eclipsejetty.additivity = false

logger.sparkstorage.name = org.apache.spark.storage
logger.sparkstorage.level = WARN
logger.sparkstorage.additivity = false

logger.sparkscheduler.name = org.apache.spark.scheduler
logger.sparkscheduler.level = WARN
logger.sparkscheduler.additivity = false

logger.sparkexecutor.name = org.apache.spark.executor
logger.sparkexecutor.level = WARN
logger.sparkexecutor.additivity = false

logger.sparknetwork.name = org.apache.spark.network
logger.sparknetwork.level = WARN
logger.sparknetwork.additivity = false

logger.sparkutil.name = org.apache.spark.util
logger.sparkutil.level = WARN
logger.sparkutil.additivity = false

logger.sparksqlcatalyst.name = org.apache.spark.sql.catalyst
logger.sparksqlcatalyst.level = WARN
logger.sparksqlcatalyst.additivity = false

# Performance monitoring
logger.datasources.name = org.apache.spark.sql.execution.datasources
logger.datasources.level = INFO
logger.datasources.additivity = false

logger.adaptive.name = org.apache.spark.sql.execution.adaptive
logger.adaptive.level = INFO
logger.adaptive.additivity = false

# Iceberg specific logging
logger.icebergspark.name = org.apache.iceberg.spark
logger.icebergspark.level = INFO
logger.icebergspark.additivity = false

logger.icebergaws.name = org.apache.iceberg.aws
logger.icebergaws.level = INFO
logger.icebergaws.additivity = false

# S3/MinIO logging
logger.s3a.name = org.apache.hadoop.fs.s3a
logger.s3a.level = INFO
logger.s3a.additivity = false

logger.s3.name = org.apache.hadoop.fs.s3
logger.s3.level = INFO
logger.s3.additivity = false

# Application-specific loggers (adjust based on your app names)
logger.dataprocessor.name = DataProcessor
logger.dataprocessor.level = INFO
logger.dataprocessor.appenderRefs = hybrid
logger.dataprocessor.appenderRef.hybrid.ref = hybrid
logger.dataprocessor.additivity =